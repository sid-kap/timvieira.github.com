<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Graduate Descent</title><link href="http://timvieira.github.io/blog/" rel="alternate"></link><link href="/blog/atom.xml" rel="self"></link><id>http://timvieira.github.io/blog/</id><updated>2014-02-12T00:00:00-05:00</updated><entry><title>Visualizing high-dimensional functions with cross-sections</title><link href="http://timvieira.github.io/blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/" rel="alternate"></link><updated>2014-02-12T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io/blog,2014-02-12:post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</id><summary type="html">&lt;p&gt;Last September, I gave a talk which included a bunch of two-dimensional plots of
a high-dimensional objective I was developing specialized algorithms for
optimizing. A month later, at least three of my colleagues told me that my plots
had inspired them to make similar plots. The plotting trick is really simple and
not original, but nonetheless I'll still write it up for all to enjoy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example plot&lt;/strong&gt;: This image shows cross-sections of two related functions: a
non-smooth (black) and a smooth approximating function (blue). The plot shows
that the approximation is faithful to the overall shape, but sometimes
over-smooths. In this case, we miss the maximum, which happens near the middle
of the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/blog/images/cross-section.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Let $f: \mathbb{R}^d \rightarrow \mathbb{R}$ be a high-dimensional
function ($d \gg 2$), which you'd like to visualize. Unfortunately, you are like
me and can't see in high-dimensions what do you do?&lt;/p&gt;
&lt;p&gt;One simple thing to do is take a nonzero vector $\boldsymbol{d} \in
\mathbb{R}^d$, take a point of interest $\boldsymbol{x}$, and build a local
picture of $f$ by evaluating it at various intervals along the chosen direction
as follows,&lt;/p&gt;
&lt;p&gt;$f_i = f(\boldsymbol{x} + \alpha_i \ \boldsymbol{d}) \ \ $ for $\alpha_i \in [\alpha_\min, \alpha_\max]$&lt;/p&gt;
&lt;p&gt;Of course, you'll have to pick a reasonable range and discretize it. Note,
$\boldsymbol{x}$ and $\boldsymbol{d}$ are fixed for all $\alpha_i$. Now, you can
plot $(\alpha_i,f_i)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Picking directions&lt;/strong&gt;: There are many alternatives for picking
$\boldsymbol{d}$, my favorites are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gradient (if it exists), this direction is guaranteed to show a local
    increase/decrease in the objective, unless it's zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Coordinate vectors. Varying one dimension per plot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random. I recommend directions drawn from a spherical
    Gaussian.&lt;sup id="fnref:sphericalgaussian"&gt;&lt;a class="footnote-ref" href="#fn:sphericalgaussian" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; The reason being that such a vector is
    uniformly distributed across all unit-length directions (i.e., the angle of
    the vector, not it's length). We will vary the length ourselves via
    $\alpha$. It's probably best that our plots don't randomly vary in scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Extension to 3d&lt;/strong&gt;: It's pretty easy to extend this generating 3d plots by
using 2 vectors, $\boldsymbol{d_1}$ and $\boldsymbol{d_2}$, and varying two
parameters $\alpha$ and $\beta$,&lt;/p&gt;
&lt;p&gt;$$
f(\boldsymbol{x} + \alpha \ \boldsymbol{d_1} + \beta \ \boldsymbol{d_2})
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing remarks&lt;/strong&gt;: These types of plots are probably best used to: empirically
verify/explore properties of an objective function, compare approximations, test
sensitivity to certain parameters/hyperparameters, visually debug optimization
algorithms.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:sphericalgaussian"&gt;
&lt;p&gt;More formally, vectors drawn from a spherical Gaussian are
points uniformly distributed on the surface of a $d$-dimensional unit sphere,
$\mathbb{S}^d$. Sampling a vector from a spherical Gaussian is straightforward:
sample $\boldsymbol{d'} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})$,
$\boldsymbol{d} = \boldsymbol{d'} / \| \boldsymbol{d'} \|_2$&amp;#160;&lt;a class="footnote-backref" href="#fnref:sphericalgaussian" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="math"></category></entry><entry><title>Exp-normalize trick</title><link href="http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/" rel="alternate"></link><updated>2014-02-11T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io/blog,2014-02-11:post/2014/02/11/exp-normalize-trick/</id><summary type="html">&lt;p&gt;This trick is the very close cousin of the infamous log-sum-exp trick
(&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html"&gt;scipy.misc.logsumexp&lt;/a&gt;),&lt;/p&gt;
&lt;p&gt;Supposed you'd like to evaluate a probability distribution $\boldsymbol{\pi}$
parametrized by a vector $\boldsymbol{x} \in \mathbb{R}^n$ as follows:&lt;/p&gt;
&lt;p&gt;$$
\pi_i = \frac{ \exp(x_i) }{ \sum_{j=1}^n \exp(x_j) }
$$&lt;/p&gt;
&lt;p&gt;The exp-normalize trick leverages the following identity to avoid numerical
overflow. For any $b \in \mathbb{R}$,&lt;/p&gt;
&lt;p&gt;$$
\pi_i
= \frac{ \exp(x_i - b) \exp(b) }{ \sum_{j=1}^n \exp(x_j - b) \exp(b) }
= \frac{ \exp(x_i - b) }{ \sum_{j=1}^n \exp(x_j - b) }
$$&lt;/p&gt;
&lt;p&gt;In other words, the $\boldsymbol{\pi}$ is shift-invariant. A reasonable choice
is $b = \max_{i=1}^n x_i$. With this choice, overflow due to $\exp$ is
impossible$-$the largest number exponentiated after shifting is $0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exp-normalize v. log-sum-exp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If what you want to remain in log-space, that is, compute
$\log(\boldsymbol{\pi})$, you should use logsumexp. However, if
$\boldsymbol{\pi}$ is your goal, then exp-normalize trick is for you! Since it
avoids additional calls to $\exp$, which would be required if using log-sum-exp.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log-sum-exp for computing the log-distibution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\log \pi_i = x_i - \mathrm{logsumexp}(\boldsymbol{x})
$$&lt;/p&gt;
&lt;p&gt;where
$$
\mathrm{logsumexp}(\boldsymbol{x}) = b + \log \sum_{j=1}^n \exp(x_j - b)
$$&lt;/p&gt;
&lt;p&gt;Typically with the same choice for $b$ as above.&lt;/p&gt;</summary><category term="math"></category></entry><entry><title>Gradient-vector product</title><link href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/" rel="alternate"></link><updated>2014-02-10T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io/blog,2014-02-10:post/2014/02/10/gradient-vector-product/</id><summary type="html">&lt;p&gt;{% notebook gradient-vector-product.ipynb cells[1:] %}&lt;/p&gt;</summary><category term="math"></category></entry></feed>