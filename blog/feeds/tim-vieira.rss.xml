<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Graduate Descent</title><link>http://timvieira.github.io/blog/</link><description></description><atom:link href="/blog/feeds/tim-vieira.rss.xml" rel="self"></atom:link><lastBuildDate>Fri, 27 May 2016 00:00:00 -0400</lastBuildDate><item><title>Dimensional analysis of gradient descent</title><link>http://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-descent/</link><description>&lt;p&gt;In physical sciences, numbers are paired with units and called quantities. In
this augmented number system, dimensional analysis provides a crucial sanity
check, much like type checking in a programming language. There are simple rules
for building up units and constraints on what operations are allowed. For
example, you can't multiply quantities which are not conformable or add
quantities with different units. Also, we generally know the units of the input
and desired output, which allows us to check that our computations at least
produce the right units.&lt;/p&gt;
&lt;p&gt;In this post, we'll discuss the dimensional analysis of gradient ascent, which
will hopefully help us understand why the "step size" is parameter so finicky
and why it even exists.&lt;/p&gt;
&lt;p&gt;Gradient ascent is an iterative procedure for (locally) maximizing a function,
&lt;span class="math"&gt;\(f: \mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
x_{t+1} = x_t + \alpha \frac{\partial f(x_t)}{\partial x}
$$&lt;/div&gt;
&lt;p&gt;In general, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a &lt;span class="math"&gt;\(d \times d\)&lt;/span&gt; matrix, but often we constrain the matrix
to be simple, e.g., &lt;span class="math"&gt;\(a\cdot I\)&lt;/span&gt; for some scalar &lt;span class="math"&gt;\(a\)&lt;/span&gt; or &lt;span class="math"&gt;\(\text{diag}(a)\)&lt;/span&gt; for some
vector &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at the units of the change in &lt;span class="math"&gt;\(\Delta x=x_{t+1} - x_t\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\Delta x) = \left(\textbf{units }\alpha\cdot \frac{\partial f(x_t)}{\partial x}\right) = (\textbf{units }\alpha) \frac{(\textbf{units }f)}{(\textbf{units }x)}.
$$&lt;/div&gt;
&lt;p&gt;The units of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; must be &lt;span class="math"&gt;\((\textbf{units }x)\)&lt;/span&gt;. However, if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt;
is unit free, we're happy with &lt;span class="math"&gt;\((\textbf{units }x) / (\textbf{units }f)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Solving for the units of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; we get,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\alpha) = \frac{(\textbf{units }x)^2}{(\textbf{units }f)}.
$$&lt;/div&gt;
&lt;p&gt;This gives us an idea for what &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; should be.&lt;/p&gt;
&lt;p&gt;For example, the inverse Hessian passes the unit check (if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt; unit
free). The disadvantages of the Hessian is that it needs to be positive-definite
(or at least invertible) in order to be a valid "step size" (i.e., we need
step sizes to be &lt;span class="math"&gt;\(&amp;gt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Another method for handling step sizes is line search. However, line search
won't let us run online. Furthermore, line search would be too slow in the case
where we want a step size for each dimension.&lt;/p&gt;
&lt;p&gt;In machine learning, we've become fond to online methods, which adapt the step
size as they go. The general idea is to estimate a step size matrix that passes
the unit check (for each dimension of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). Furthermore, we want do as little
extra work as possible to get this estimate (e.g., we want to avoid computing a
Hessian because that would be extra work). So, the step size should be based
only iterates and gradients up to time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf"&gt;AdaGrad&lt;/a&gt; doesn't doesn't
  pass the unit check. This motivated AdaDelta.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1212.5701"&gt;AdaDelta&lt;/a&gt; uses the ratio of (running
  estimates of) the root-mean-squares of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\partial f / \partial
  x\)&lt;/span&gt;. The mean is taken using an exponentially weighted moving average. See
  paper for actual implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; came later and made some tweaks to
  remove (unintended) bias in the AdaDelta estimates of the numerator and
  denominator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, it's important/useful to analyze the units of numerical algorithms
in order to get a sanity check (i.e., catch mistakes) as well as to develop an
understanding of why certain parameters exist and how properties of a problem
affect the values we should use for them.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Fri, 27 May 2016 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2016-05-27:blog/post/2016/05/27/dimensional-analysis-of-gradient-descent/</guid><category>math</category><category>optimization</category></item><item><title>Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</title><link>http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/</link><description>&lt;p&gt;The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization---the "inner" optimization optimizes training loss (wrt &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;),
while the "outer" optimizes hyperparameters (&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$&lt;/div&gt;
&lt;p&gt;Can we estimate &lt;span class="math"&gt;\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)&lt;/span&gt; so
that we can run gradient-based optimization over &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Well, what does it mean to have an &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; inside a function?&lt;/p&gt;
&lt;p&gt;Well, it means that there is a &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that gets passed to
&lt;span class="math"&gt;\(\mathcal{L}_{\text{dev}}\)&lt;/span&gt;. And, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, denoted
&lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt;. Furthermore, &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt; is the solution to,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$&lt;/div&gt;
&lt;p&gt;Now how does &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; change as the result of an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The constraint on the derivative implies a type of "equilibrium"---the inner
optimization process will continue to optimize regardless of how we change
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Assuming we don't change &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; too much, then the inner
optimization shouldn't change &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; too much and it will change in a
predictable way.&lt;/p&gt;
&lt;p&gt;To do this we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are related through a
function &lt;span class="math"&gt;\(g\)&lt;/span&gt; as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$g(x,y) = 0.$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;span class="math"&gt;\(g\)&lt;/span&gt; is a smooth function in &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;, then we can perturb either
argument, say &lt;span class="math"&gt;\(x\)&lt;/span&gt; by a small amount &lt;span class="math"&gt;\(\Delta_x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; by &lt;span class="math"&gt;\(\Delta_y\)&lt;/span&gt;. Because
system preserves the constraint, i.e.,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$&lt;/div&gt;
&lt;p&gt;We can solve for the change of &lt;span class="math"&gt;\(x\)&lt;/span&gt; as a result of an infinitesimal change in
&lt;span class="math"&gt;\(y\)&lt;/span&gt;. We take the first-order expansion,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(g(x,y)\)&lt;/span&gt; is already zero,&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Next, we solve for &lt;span class="math"&gt;\(\frac{\Delta_x}{\Delta_y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$&lt;/div&gt;
&lt;p&gt;Back to the original problem: Now we can use the implicit function theorem to
estimate how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; varies in &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; by plugging in &lt;span class="math"&gt;\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)&lt;/span&gt;, &lt;span class="math"&gt;\(x \mapsto \theta\)&lt;/span&gt;
and &lt;span class="math"&gt;\(y \mapsto \lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$&lt;/div&gt;
&lt;p&gt;This tells us how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; changes with respect to an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Now, we can apply the chain rule we get the gradient of the whole
optimization problem,&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$&lt;/div&gt;
&lt;p&gt;Since we don't like (explicit) matrix inverses, we compute &lt;span class="math"&gt;\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top }
\right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)&lt;/span&gt; as the solution to &lt;span class="math"&gt;\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x =
-\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)&lt;/span&gt;. When the Hessian is positive definite we can solve this linear system
with conjugate gradient, which conveniently only requires matrix-vector
products---i.e., you never have to materialize the Hessian. (Apparently,
&lt;a href="https://en.wikipedia.org/wiki/Matrix-free_methods"&gt;matrix-free linear algebra&lt;/a&gt; is a thing.)&lt;/p&gt;
&lt;p&gt;In fact, both the Hessian-vector product and the Jacobian-vector products are
accurately and efficiently approximated with centered differences (see
&lt;a href="/blog/post/2014/02/10/gradient-vector-product/"&gt;earlier post&lt;/a&gt;), which means you
don't even need to write code to compute the Hessian or Jacobian---you just need
the first derivatives.&lt;/p&gt;
&lt;p&gt;At the end of the day, this is a pretty easy an algorithm! However, the estimate
of the gradient we get can temperamental if the linear system is
ill-conditioned.&lt;/p&gt;
&lt;p&gt;In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through gradient descent (or other optimization algorithms),
which make fewer and less-brittle assumptions about the inner optimization.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sat, 05 Mar 2016 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2016-03-05:blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/</guid><category>math</category><category>calculus</category></item><item><title>Multidimensional array index</title><link>http://timvieira.github.io/blog/post/2016/01/17/multidimensional-array-index/</link><description>&lt;p&gt;This is a simple note on how to compute a bijective mapping between the indices
of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional array and a flat, one-dimensional array. We'll look at
both directions of the mapping: &lt;code&gt;(tuple-&amp;gt;int)&lt;/code&gt; and &lt;code&gt;(int -&amp;gt; tuple)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We'll assume each dimension &lt;span class="math"&gt;\(a, b, c, \ldots\)&lt;/span&gt; is a positive integer and bounded
&lt;span class="math"&gt;\(a \le A, b \le B, c \le C, \ldots\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Start small&lt;/h3&gt;
&lt;p&gt;Let's start by looking at &lt;span class="math"&gt;\(n = 3\)&lt;/span&gt; and generalize from there.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's our test case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: This is not the only bijective mapping from &lt;code&gt;tuple&lt;/code&gt; to &lt;code&gt;int&lt;/code&gt; that we
could have come up with. The one we chose corresponds to the particular layout,
which is apparent in the test case.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(n=4\)&lt;/span&gt; the pattern is &lt;span class="math"&gt;\(((a \cdot B + b) \cdot C + d) \cdot D + d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Sidenote: We don't actually need the bound &lt;span class="math"&gt;\(a \le A\)&lt;/span&gt; in either &lt;code&gt;index&lt;/code&gt; or
&lt;code&gt;inverse&lt;/code&gt;. This gives us a little extra flexibility because our first
dimension can be infinite/unknown.&lt;/p&gt;
&lt;h3&gt;General case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Map tuple ``a`` to index with known bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# the pattern:&lt;/span&gt;
    &lt;span class="c1"&gt;# ((i*J + j)*K + k)*L + l&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Find key given index ``ix`` and bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Testing the general case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Test that `index` produces the layout we expect.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;got&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;got&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 17 Jan 2016 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2016-01-17:blog/post/2016/01/17/multidimensional-array-index/</guid><category>misc</category></item><item><title>Gradient of a product</title><link>http://timvieira.github.io/blog/post/2015/07/29/gradient-of-a-product/</link><description>&lt;div class="math"&gt;$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$&lt;/div&gt;
&lt;p&gt;In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).&lt;/p&gt;
&lt;p&gt;Given a collection of functions with a common argument &lt;span class="math"&gt;\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define their product &lt;span class="math"&gt;\(p(x) = \prod_{i=1}^n f_i(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."&lt;/p&gt;
&lt;p&gt;A naive method for computing the gradient computes the leave-one-out products
from scratch for each &lt;span class="math"&gt;\(i\)&lt;/span&gt; (outer loop)---resulting in a overall runtime of
&lt;span class="math"&gt;\(O(n^2)\)&lt;/span&gt; to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Division trick&lt;/strong&gt;: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f \ne 0\)&lt;/span&gt;. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requires multiplicative inverse operator (division) &lt;em&gt;and&lt;/em&gt;
   associative-commutative multiplication, which means it's not applicable to
   matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Log trick&lt;/strong&gt;: Suppose &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;). It's common practice (especially in machine learning) to replace
&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log f_i\)&lt;/span&gt;, which turns products into sums, &lt;span class="math"&gt;\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)&lt;/span&gt;, and tiny numbers (like
&lt;span class="math"&gt;\(\texttt{3.72e-44}\)&lt;/span&gt;) into large ones (like &lt;span class="math"&gt;\(\texttt{-100}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Furthermore, using the identity &lt;span class="math"&gt;\((\nabla g) = g \cdot \nabla \log g\)&lt;/span&gt;, we can
operate exclusively in the "&lt;span class="math"&gt;\(\log\)&lt;/span&gt;-domain".&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Numerically stable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't require multiplicative inverse assuming you can compute &lt;span class="math"&gt;\(\gradx{ \log
   f_i(x) }\)&lt;/span&gt; without it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f &amp;gt; 0\)&lt;/span&gt;. But, we can use
   &lt;a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/"&gt;LogReal number class&lt;/a&gt;
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't easily generalize to other notions of multiplication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic programming trick&lt;/strong&gt;: &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; runtime and &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. &lt;a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf"&gt;Wallach (2004)&lt;/a&gt;,
section 7).&lt;/p&gt;
&lt;p&gt;The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.&lt;/p&gt;
&lt;p&gt;Here are the equations:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\alpha_0(x) &amp;amp;=&amp;amp; 1 \\
\alpha_t(x)
   &amp;amp;=&amp;amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;amp;=&amp;amp; 1 \\
\beta_t(x)
  &amp;amp;=&amp;amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \prod_{j &amp;lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &amp;gt; i} f_j(x) \right) \\
&amp;amp;=&amp;amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Clearly, this requires &lt;span class="math"&gt;\(O(n)\)&lt;/span&gt; additional space.&lt;/p&gt;
&lt;p&gt;Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).&lt;/p&gt;
&lt;p&gt;Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where &lt;span class="math"&gt;\(A B C \ne B C A\)&lt;/span&gt;, even if all matrices have the
conformable dimensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Connections to automatic differentiation&lt;/strong&gt;: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
&lt;em&gt;can&lt;/em&gt; compute it's gradient with the same asymptotic complexity, &lt;em&gt;but&lt;/em&gt; you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with &lt;span class="math"&gt;\(\bigo(n^2)\)&lt;/span&gt; time and &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt; space
(other than the space to store the &lt;span class="math"&gt;\(n\)&lt;/span&gt; functions) and ended up with a &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt;
time &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space algorithm with a little clever thinking. What I'm saying
is autodiff---even if you don't use a magical package---tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sketch&lt;/em&gt;: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). It turns out that the backward pass gives us &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as
adjoints. Lastly, we'd propagate gradients through the &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s to get
&lt;span class="math"&gt;\(\frac{\partial p}{\partial x}\)&lt;/span&gt;. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Wed, 29 Jul 2015 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2015-07-29:blog/post/2015/07/29/gradient-of-a-product/</guid><category>math</category><category>calculus</category><category>numerical</category></item><item><title>Multiclass logistic regression and conditional random fields are the same thing</title><link>http://timvieira.github.io/blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/</link><description>&lt;p&gt;A short rant: multiclass logistic regression and conditional random fields (CRF)
are the same thing. This comes to a surprise to many people because CRFs tend to
be surrounded by additional "stuff."&lt;/p&gt;
&lt;p&gt;Multiclass logistic regression is simple. The goal is to predict the correct
label &lt;span class="math"&gt;\(y^*\)&lt;/span&gt; from handful of labels &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt; given the observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; based
on features &lt;span class="math"&gt;\(\phi(x,y)\)&lt;/span&gt;. Training this model typically requires computing the
gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,y^*) - \sum_{y \in \mathcal{Y}} p(y|x) \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
p(y|x) &amp;amp;=&amp;amp; \frac{1}{Z(x)} \exp(\theta^\top \phi(x,y)) &amp;amp; \ \ \ \ \text{and} \ \ \ \ &amp;amp;
Z(x) &amp;amp;=&amp;amp; \sum_{y \in \mathcal{Y}} \exp(\theta^\top \phi(x,y))
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;At test-time, we often take the highest-scoring label under the model.&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{y}(x) = \textbf{argmax}_{y \in \mathcal{Y}} \theta^\top \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;A conditional random field is exactly multiclass logistic regression. The only
difference is that the sum and argmax are inefficient to compute naively (i.e.,
by enumeration). This point is often lost when people first learn about
CRFs. Some people never make this connection.&lt;/p&gt;
&lt;p&gt;Here's some stuff you'll see once we start talking about CRFs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inference algorithms (e.g., Viterbi decoding, forward-backward, Junction
   tree)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graphical models (factor graphs, Bayes nets, Markov random fields)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model templates (i.e., repeated feature functions)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the logistic regression case, we'd never use the term "inference" to describe
the "sum" and "max" over a handful of categories. Once we move to a structured
label space, this term gets throw around. (BTW, this isn't "statistical
inference," just algorithms to compute sum and max over &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;Graphical models establish a notation and structural properties which allow
efficient inference -- things like cycles and treewidth.&lt;/p&gt;
&lt;p&gt;Model templating is the only essential trick to move from logistic regression to
a CRF. Templating "solves" the problem that not all training examples have the
same "size" -- the set of outputs &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; now depends on &lt;span class="math"&gt;\(x\)&lt;/span&gt;. A model
template specifies how to compute the features for an entire output, by looking
at interactions between subsets of variables.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,\boldsymbol{y}) = \sum_{\alpha \in A(x)} \phi_\alpha(x,
\boldsymbol{y}_\alpha)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a labeled subset of variables often called a factor and
&lt;span class="math"&gt;\(\boldsymbol{y}_\alpha\)&lt;/span&gt; is the subvector containing values of variables
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. Basically, the feature function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gets to look at some subset of
the variables being predicted &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the entire input &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The ability to look
at more of &lt;span class="math"&gt;\(y\)&lt;/span&gt; allows the model to make more coherent predictions.&lt;/p&gt;
&lt;p&gt;Anywho, it's often useful to take a step back and think about what you are
trying to compute instead of how you're computing it. In this post, this allowed
us see the similarity between logistic regression and CRFs even though they seem
quite different.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Wed, 29 Apr 2015 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2015-04-29:blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/</guid><category>machine-learning</category><category>rant</category><category>crf</category></item><item><title>Conditional random fields as Deep learning models?</title><link>http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/</link><description>&lt;p&gt;This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.&lt;/p&gt;
&lt;p&gt;In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Limit the expressiveness of the model (i.e., don't use to model you want)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the training objective&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approximate inference&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Approximate inference is tricky. Things can easily go awry.&lt;/p&gt;
&lt;p&gt;For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
&lt;a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf"&gt;(Kulesza &amp;amp; Pereira, 2007)&lt;/a&gt;. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
&lt;a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf"&gt;sometimes&lt;/a&gt; these loopy BP
approximations works fine.)&lt;/p&gt;
&lt;p&gt;It looks like using approximate inference during training changes the training
objective.&lt;/p&gt;
&lt;p&gt;So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.&lt;/p&gt;
&lt;p&gt;Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp;amp; Eisner.&lt;/p&gt;
&lt;p&gt;Unsuprisingly, it works pretty well.&lt;/p&gt;
&lt;p&gt;I first saw this idea in Stoyanov &amp;amp; Eisner (2011). They use loopy belief
propagation as their approximate inference algorithm. At the end of the day,
their model is essentially a deep recurrent network, which came from unrolling
inference in a graphical model. This idea really struck me because it's clearly
right in the middle between graphical models and deep learning.&lt;/p&gt;
&lt;p&gt;You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.&lt;/p&gt;
&lt;p&gt;Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Thu, 05 Feb 2015 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2015-02-05:blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/</guid><category>machine-learning</category><category>deep-learning</category><category>structured-prediction</category></item><item><title>Log-Real number class</title><link>http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/</link><description>&lt;p&gt;Most people know how to avoid numerical underflow in probability computations by
representing intermediate quantities in the log-domain. This trick turns
"multiplication" into "addition", "addition" into "logsumexp", "0" into
&lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; and "1" into &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Most importantly, it turns really small numbers into
reasonable-size numbers.&lt;/p&gt;
&lt;p&gt;Unfortunately, without modification, this trick is limited to positive numbers
because &lt;code&gt;log&lt;/code&gt; of a negative number is &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Well, there is good news! For the cost of an extra bit, we can extend this trick
to the negative reals and furthermore, we get a bonafide ring instead of a mere
semiring.&lt;/p&gt;
&lt;p&gt;I first saw this trick in
&lt;a href="http://www.aclweb.org/anthology/D09-1005"&gt;Li and Eisner (2009)&lt;/a&gt;. The trick is
nicely summarized in Table 3 of that paper, which I've pasted below.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
&lt;img src="/blog/images/logreal.png"/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Why do I care?&lt;/strong&gt; When computing gradients (e.g., gradient of risk),
intermediate values are rarely all positive. Furthermore, we're often
multiplying small things together. I've recently found log-reals to be effective
at squeaking a bit more numerical accuracy.&lt;/p&gt;
&lt;p&gt;This trick is useful for almost all backprop computations because backprop is
essentially:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;adjoint(u) += adjoint(v) * dv/du.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only tricky bit is lifting all &lt;code&gt;du/dv&lt;/code&gt; computations into the log-reals.&lt;/p&gt;
&lt;p&gt;Implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This trick is better suited to programming languages with structs. Using
  objects will probably in an horrible slow down and using parallel arrays to
  store the sign bit and double is probably too tedious and error prone. (Sorry
  java folks.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here's a &lt;a href="https://github.com/andre-martins/TurboParser/blob/master/src/util/logval.h"&gt;C++ implementation&lt;/a&gt;
  with operator overloading from Andre Martins&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that log-real &lt;code&gt;+=&lt;/code&gt; involves calls to &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;, which will
  definitely slow your code down a bit (these functions are much slower than
  addition).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 01 Feb 2015 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2015-02-01:blog/post/2015/02/01/log-real-number-class/</guid><category>math</category><category>numerical</category></item><item><title>Importance Sampling</title><link>http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/</link><description>&lt;p&gt;Importance sampling is a powerful and pervasive technique in statistics, machine
learning and randomized algorithms.&lt;/p&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Importance sampling is a technique for estimating the expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of a
random variable &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; under distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; from samples of a different
distribution &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The key observation is that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is can expressed as the expectation of a
different random variable &lt;span class="math"&gt;\(f^*(x)=\frac{p(x)}{q(x)}\! \cdot\! f(x)\)&lt;/span&gt; under &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}_{q}\! \left[ f^*(x) \right] = \mathbb{E}_{q}\! \left[ \frac{p(x)}{q(x)} f(x) \right] = \sum_{x} q(x) \frac{p(x)}{q(x)} f(x) = \sum_{x} p(x) f(x) = \mathbb{E}_{p}\! \left[ f(x) \right] = \mu
$$&lt;/div&gt;
&lt;p&gt;Technical condition: &lt;span class="math"&gt;\(q\)&lt;/span&gt; must have support everywhere &lt;span class="math"&gt;\(p\)&lt;/span&gt; does, &lt;span class="math"&gt;\(p(x) &amp;gt; 0
\Rightarrow q(x) &amp;gt; 0\)&lt;/span&gt;. Without this condition, the equation is biased! Note: &lt;span class="math"&gt;\(q\)&lt;/span&gt;
can support things that &lt;span class="math"&gt;\(p\)&lt;/span&gt; doesn't.&lt;/p&gt;
&lt;p&gt;Terminology: The quantity &lt;span class="math"&gt;\(w(x) = \frac{p(x)}{q(x)}\)&lt;/span&gt; is often referred to as the
"importance weight" or "importance correction". We often refer to &lt;span class="math"&gt;\(p\)&lt;/span&gt; as the
target density and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the proposal density.&lt;/p&gt;
&lt;p&gt;Now, given samples &lt;span class="math"&gt;\(\{ x^{(i)} \}_{i=1}^{n}\)&lt;/span&gt; from &lt;span class="math"&gt;\(q\)&lt;/span&gt;, we can use the Monte
Carlo estimate, &lt;span class="math"&gt;\(\hat{\mu} \approx \frac{1}{n} \sum_{i=1}^n f^{*}(x^{(i)})\)&lt;/span&gt;, as
an unbiased estimator of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;There are a few reasons we might want use importance sampling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convenience&lt;/strong&gt;: It might be trickier to sample directly from &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bias-correction&lt;/strong&gt;: Suppose, we're developing an algorithm which requires
     samples to satisfy some "safety" condition (e.g., a minimum support
     threshold) and be unbiased. Importance sampling can be used to remove bias,
     while satisfying the condition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: It might be the case that sampling directly from
     &lt;span class="math"&gt;\(p\)&lt;/span&gt; would require more samples to estimate &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. Check out these
     &lt;a href="http://www.columbia.edu/~mh2078/MCS04/MCS_var_red2.pdf"&gt;great notes&lt;/a&gt; for
     more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-policy evaluation and learning&lt;/strong&gt;: We might want to collect some
     "exploratory data" from &lt;span class="math"&gt;\(q\)&lt;/span&gt; and evaluate different "policies", &lt;span class="math"&gt;\(p\)&lt;/span&gt; (e.g.,
     to pick the best one). Some cool papers:
     &lt;a href="http://arxiv.org/abs/1209.2355"&gt;counterfactual reasoning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;reinforcement learning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/1103.4601"&gt;contextual bandits&lt;/a&gt;,
     &lt;a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;domain adaptation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few common cases for &lt;span class="math"&gt;\(q\)&lt;/span&gt; worth separate consideration:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: This is the case in experimental design, variance
     reduction, active learning and reinforcement learning. It's often difficult
     to design &lt;span class="math"&gt;\(q\)&lt;/span&gt;, which results in an estimator with "reasonable" variance. A
     very difficult case is in off-policy evaluation because it (essentially)
     requires a good exploratory distribution for every possible policy. (I have
     much more to say on this topic.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Little to no control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: For example, you're given some dataset
     (e.g., new articles) and you want to estimate performance on a different
     dataset (e.g., Twitter).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: In this case, we want to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; (typically referred
     to as the propensity score) and use it in the importance sampling
     estimator. This technique, as far as I can tell, is widely used to remove
     selection bias when estimating effects of different treatments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;: The main drawback of importance sampling is variance. A few bad
samples with large weights can drastically throw off the estimator. Thus, it's
often the case that a biased estimator is preferred, e.g.,
&lt;a href="https://hips.seas.harvard.edu/blog/2013/01/14/unbiased-estimators-of-partition-functions-are-basically-lower-bounds/"&gt;estimating the partition function&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/1209.2355"&gt;clipping weights&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/cs/0204043"&gt;indirect importance sampling&lt;/a&gt;. A secondary
drawback is that both densities must be normalized, which is often intractable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's next?&lt;/strong&gt; I plan to cover "variance reduction" and "off-policy
evaluation" in more detail in future posts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 21 Dec 2014 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2014-12-21:blog/post/2014/12/21/importance-sampling/</guid><category>math</category><category>statistics</category><category>randomized</category></item><item><title>Numerically-stable p-norms</title><link>http://timvieira.github.io/blog/post/2014/11/10/numerically-stable-p-norms/</link><description>&lt;p&gt;Consider the p-norm
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$&lt;/div&gt;
&lt;p&gt;In python this translates to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;First-pass implementation of p-norm.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, suppose &lt;span class="math"&gt;\(|x_i|^p\)&lt;/span&gt; causes overflow (for some &lt;span class="math"&gt;\(i\)&lt;/span&gt;). This will occur for sufficiently large &lt;span class="math"&gt;\(p\)&lt;/span&gt; or sufficiently large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;---even if &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is representable (i.e., not NaN or &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;big&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e300&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c1"&gt;# expected: 1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This fails because we can't square big&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;A little math&lt;/h2&gt;
&lt;p&gt;There is a way to avoid overflow on the account of a few large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here's a little fact about p-norms: for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \alpha \cdot \boldsymbol{x} ||_p = |\alpha| \cdot || \boldsymbol{x}   ||_p
$$&lt;/div&gt;
&lt;p&gt;We'll use the following version (harder to remember)
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p  = |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
$$&lt;/div&gt;
&lt;p&gt;Don't believe it? Here's some algebra:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
|| \boldsymbol{x} ||_p
&amp;amp;=&amp;amp; \left( \sum_i |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; \left( \sum_i \frac{|\alpha|^p}{|\alpha|^p} \cdot |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left( \frac{|x_i| }{|\alpha|} \right)^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left| \frac{x_i }{\alpha} \right|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
\end{eqnarray*}
$$&lt;/div&gt;
&lt;h2&gt;Back to numerical stability&lt;/h2&gt;
&lt;p&gt;Suppose we pick &lt;span class="math"&gt;\(\alpha = \max_i |x_i|\)&lt;/span&gt;. Now, the largest number we have to take
the power of is one --- making it very difficult to overflow on the account of
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. This should remind you of the infamous log-sum-exp trick.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, our example from before works :-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It appears as if &lt;code&gt;scipy.linalg.norm&lt;/code&gt; is robust to overflow, while &lt;code&gt;numpy.linalg.norm&lt;/code&gt; is not. Note that &lt;code&gt;scipy.linalg.norm&lt;/code&gt; appears to be a bit slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;logsumexp&lt;/code&gt; trick is nearly identical, but operates in the log-domain, i.e., &lt;span class="math"&gt;\(\text{logsumexp}(\log(|x|) \cdot p) / p = \log || x ||_p\)&lt;/span&gt;. You can implement both tricks with the same code, if you use different number classes for log-domain and real-domain---a trick you might have seen before.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.math&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 10 Nov 2014 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2014-11-10:blog/post/2014/11/10/numerically-stable-p-norms/</guid><category>math</category><category>numerical</category></item><item><title>KL-divergence as an objective function</title><link>http://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/</link><description>&lt;p&gt;It's well-known that
&lt;a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL-divergence&lt;/a&gt;
is not symmetric, but which direction is right for fitting your model?&lt;/p&gt;
&lt;p&gt;First, which one is which?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cheat sheet&lt;/strong&gt;: If we're fitting &lt;span class="math"&gt;\(q\)&lt;/span&gt; to &lt;span class="math"&gt;\(p\)&lt;/span&gt; using&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(q || p)\)&lt;/span&gt;
  - mode-seeking, exclusive
  - no normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(p || q)\)&lt;/span&gt;
  - mean-seeking, inclusive
  - results in a moment-matching problem when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family
  - requires normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How I remember which is which&lt;/strong&gt;: The KL notation is a little strange with the
"&lt;span class="math"&gt;\(||\)&lt;/span&gt;". I pretend it's a division symbol. This happens to correspond nicely to a
division symbol in the corresponding equation (I'm not sure it's
intentional). It helps to remember that you always normalize wrt the "numerator"
(and &lt;span class="math"&gt;\(q\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Computational perspecive&lt;/h2&gt;
&lt;p&gt;Let's look at what's involved in fitting a model &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; in each
direction. In this section, I'll describe the gradient and pay special attention
to the issue of normalization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;: &lt;span class="math"&gt;\(p,q_\theta\)&lt;/span&gt; are probabilty distributions. &lt;span class="math"&gt;\(p = \bar{p} / Z_p\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is the normalization constant. Similarly for &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The easy direction &lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(q_\theta || p)
&amp;amp;= \sum_d q(d) \log \left( \frac{q(d)}{p(d)} \right) \\
&amp;amp;= \sum_d q(d) \left( \log q(d) - \log p(d) \right) \\
&amp;amp;= \underbrace{\sum_d q(d) \log q(d)}_{-\text{entropy}} - \underbrace{\sum_d q(d) \log p(d)}_{\text{cross-entropy}} \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's look at normalization of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, the entropy term is easy because there is no &lt;span class="math"&gt;\(p\)&lt;/span&gt; in it.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d q(d) \log p(d)
&amp;amp;= \sum_d q(d) \log (\bar{p}(d) / Z_p) \\
&amp;amp;= \sum_d q(d) \left( \log \bar{p}(d) - \log Z_p) \right) \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \sum_d q(d) \log Z_p \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \log Z_p
\end{align*}&lt;/div&gt;
&lt;p&gt;In this case, &lt;span class="math"&gt;\(-\log Z_p\)&lt;/span&gt; is an additive constant, which can be dropped because
we're optimizing.&lt;/p&gt;
&lt;p&gt;This leaves us with the following optimization problem:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \text{argmin}_\theta \textbf{KL}(q_\theta || p) \\
&amp;amp;\qquad = \text{argmin}_\theta \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d)
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's work out the gradient
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \nabla\left[ \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d) \right] \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \log q_\theta(d) \right] - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) \right) - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) - \log \bar{p}(d) \right) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( \log q_\theta(d) - \log \bar{p}(d) \right) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;We killed the one in the last equality because &lt;span class="math"&gt;\(\sum_d \nabla
\left[ q(d) \right] = \nabla \left[ \sum_d q(d) \right] = \nabla
\left[ 1 \right] = 0\)&lt;/span&gt;, for any &lt;span class="math"&gt;\(q\)&lt;/span&gt; which is a probability distribution.&lt;/p&gt;
&lt;p&gt;This direction is convenient because we don't need to normalize
&lt;span class="math"&gt;\(p\)&lt;/span&gt;. Unfortunately, the "easy" direction is nonconvex in general --- unlike the
"hard" direction, which is convex.&lt;/p&gt;
&lt;h3&gt;Harder direction &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(p || q_\theta)
&amp;amp;= \sum_d p(d) \log \left( \frac{p(d)}{q(d)} \right) \\
&amp;amp;= \sum_d p(d) \left( \log p(d) - \log q(d) \right) \\
&amp;amp;= \sum_d p(d) \log p(d) - \sum_d p(d) \log q(d) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Clearly the first term (entropy) won't matter if we're just trying optimize wrt
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. So, let's focus on the second term (cross-entropy).
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d p(d) \log q(d)
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \left( \log \bar{q}(d) - \log Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \frac{1}{Z_p} \sum_d \bar{p}(d) \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \frac{1}{Z_p} \sum_d \bar{p}(d) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q
\end{align*}&lt;/div&gt;
&lt;p&gt;Unfortunately the &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; factor is unavoidable. The usual "approximate inference
story" is that &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is hard to compute, while the approximating distributions
normalization constant &lt;span class="math"&gt;\(Z_q\)&lt;/span&gt; is easy.&lt;/p&gt;
&lt;p&gt;Nonetheless, optimizing KL in this direction is still useful. Examples include,
expectation propagation, variational decoding and maximum likelihood
estimation. In the case of maximum likelihood estimation, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the empirical
distribution, so technically you don't have to compute its normalizing constant,
but you do need to sample from it (which can be just as hard).&lt;/p&gt;
&lt;p&gt;The gradient, when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family, is intuitive:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\nabla \left[ \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \right]
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \nabla \left[ \log \bar{q}(d) \right] - \nabla \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \phi_q(d) - \mathbb{E}_q \left[ \phi_q \right] \\
&amp;amp;= \mathbb{E}_p \left[ \phi_q \right] - \mathbb{E}_q \left[ \phi_q \right]
\end{align*}&lt;/div&gt;
&lt;p&gt;Optimization problem is convex when &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; is an exponential families ---
i.e., &lt;span class="math"&gt;\(p\)&lt;/span&gt; can be arbitrary. You can think of maximum likelihood estimation as a
method which minimizes KL divergence from samples of &lt;span class="math"&gt;\(p\)&lt;/span&gt;. In this case, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is
the true data distribution! The first term in the gradient is based on a sample
instead of an exact estimate (often called "observed feature counts").&lt;/p&gt;
&lt;p&gt;Downside: computing &lt;span class="math"&gt;\(\mathbb{E}_p \left[ \phi_q \right]\)&lt;/span&gt; might not be tractable.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Both directions of KL are special cases of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence. For a unified
  account of both directions consider looking into &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inclusive divergences require &lt;span class="math"&gt;\(q &amp;gt; 0\)&lt;/span&gt; whenever &lt;span class="math"&gt;\(p &amp;gt; 0\)&lt;/span&gt;. No "false negatives".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exclusive divergences will often favor a single mode.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computing the value of KL (not just the gradient) in either direction requires
  normalization. However, in the "easy" direction, using unnormalized &lt;span class="math"&gt;\(p\)&lt;/span&gt;
  results in only an additive constant difference. So, it's still just as
  useful, if all you care about is optimization (fitting the model).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 06 Oct 2014 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2014-10-06:blog/post/2014/10/06/kl-divergence-as-an-objective-function/</guid><category>math</category><category>statistics</category><category>machine-learning</category></item><item><title>Complex-step derivative</title><link>http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/</link><description>&lt;p&gt;Estimate derivatives by simply passing in a complex number to your function!&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;Recall, the centered-difference approximation is a fairly accurate method for
approximating derivatives of a univariate function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, which only requires two
function evaluations. A similar derivation, based on the Taylor series expansion
with a complex perturbation, gives us a similarly-accurate approximation with a
single (complex) function evaluation instead of two (real-valued) function
evaluations. Note: &lt;span class="math"&gt;\(f\)&lt;/span&gt; must support complex inputs (in frameworks, such as numpy
or matlab, this often requires no modification to source code).&lt;/p&gt;
&lt;p&gt;This post is based on
&lt;a href="http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf"&gt;Martins+'03&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;: Start with the Taylor series approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x + i \cdot \varepsilon) =
  \frac{i^0 \varepsilon^0}{0!} f(x)
+ \frac{i^1 \varepsilon^1}{1!} f'(x)
+ \frac{i^2 \varepsilon^2}{2!} f''(x)
+ \frac{i^3 \varepsilon^3}{3!} f'''(x)
+ \cdots
$$&lt;/div&gt;
&lt;p&gt;Take the imaginary part of both sides and solve for &lt;span class="math"&gt;\(f'(x)\)&lt;/span&gt;. Note: the &lt;span class="math"&gt;\(f\)&lt;/span&gt; and
&lt;span class="math"&gt;\(f''\)&lt;/span&gt; term disappear because &lt;span class="math"&gt;\(i^0\)&lt;/span&gt; and &lt;span class="math"&gt;\(i^2\)&lt;/span&gt; are real-valued.&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) = \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big] + \frac{\varepsilon^2}{3!} f'''(x) + \cdots
$$&lt;/div&gt;
&lt;p&gt;As usual, using a small &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; let's us throw out higher-order
terms. And, we arrive at the following approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;If instead, we take the real part and solve for &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, we get an approximation
to the function's value at &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) \approx \text{Re}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;In other words, a single (complex) function evaluations computes both the
function's value and the derivative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Higher-order function takes univariate function which computes a value and&lt;/span&gt;
&lt;span class="sd"&gt;    returns a function which returns value-derivative pair approximation.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;         &lt;span class="c1"&gt;# convert input to complex number&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imag&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# return function value and gradient&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simple test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="c1"&gt;# function&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# gradient&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Other comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using the complex-step method to estimate the gradients of multivariate
  functions requires independent approximations for each dimension of the
  input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although the complex-step approximation only requires a single function
  evaluation, it's unlikely faster than performing two function evaluations
  because operations on complex numbers are generally much slower than on floats
  or doubles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Thu, 07 Aug 2014 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2014-08-07:blog/post/2014/08/07/complex-step-derivative/</guid><category>math</category><category>calculus</category></item><item><title>Gumbel max trick and weighted reservoir sampling</title><link>http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</link><description>&lt;p&gt;A while back, &lt;a href="http://people.cs.umass.edu/~wallach/"&gt;Hanna&lt;/a&gt; and I stumbled upon
the following blog post:
&lt;a href="http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm"&gt;Algorithms Every Data Scientist Should Know: Reservoir Sampling&lt;/a&gt;,
which got us excited about reservior sampling.&lt;/p&gt;
&lt;p&gt;Around the same time, I attended an talk by
&lt;a href="http://cs.haifa.ac.il/~tamir/"&gt;Tamir Hazan&lt;/a&gt; about some of his work on
perturb-and-MAP
&lt;a href="http://cs.haifa.ac.il/~tamir/papers/mean-width-icml12.pdf"&gt;(Hazan &amp;amp; Jaakkola, 2012)&lt;/a&gt;,
which is inspired by the
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;Gumbel-max-trick&lt;/a&gt;
(see &lt;a href="/blog/post/2014/07/31/gumbel-max-trick/"&gt;previous post&lt;/a&gt;). The apparent
similarity between weighted reservior sampling and the Gumbel max trick lead us
to make some cute connections, which I'll describe in this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We're given a stream of unnormalized probabilities, &lt;span class="math"&gt;\(x_1,
x_2, \cdots\)&lt;/span&gt;. At any point in time &lt;span class="math"&gt;\(t\)&lt;/span&gt; we'd like to have a sampled index &lt;span class="math"&gt;\(i\)&lt;/span&gt;
available, where the probability of &lt;span class="math"&gt;\(i\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(\pi_t(i) = \frac{x_i}{
\sum_{j=1}^t x_j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume, without loss of generality, that &lt;span class="math"&gt;\(x_i &amp;gt; 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. (If any element
has a zero weight we can safely ignore it since it should never be sampled.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Gumbel-max sampler&lt;/strong&gt;: I came up with the following algorithm, which
is a simple "modification" of the Gumbel-max-trick for handling streaming data:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}  \ \ \text{# maximum value and index}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Compute log-unnormalized probabilities&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(w_i = \log(x_i)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(z_i \sim \text{Gumbel}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(k_i = w_i + z_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; (i.e. the argmax)&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If we interrupt this algorithm at any point, we have a sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After convincing myself this algorithm was correct, I sat down to try to
understand the algorithm in the blog post, which is due to Efraimidis and
Spirakis (2005) (&lt;a href="http://dl.acm.org/citation.cfm?id=1138834"&gt;paywall&lt;/a&gt;,
&lt;a href="http://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf"&gt;free summary&lt;/a&gt;). They
looked similar in many ways but used different sorting keys / perturbations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efraimidis and Spirakis (2005)&lt;/strong&gt;: Here is the ES algorithm for weighted
reservior sampling&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate,&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(u_i \sim \text{Uniform}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(e_i = u_i^{(\frac{1}{x_i})}\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Again, if interrupt this algorithm at any point, we have our sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: Let's try to relate these algorithms. At a high level, both
algorithms compute a randomized key and take an argmax. What's the relationship
between the keys?&lt;/p&gt;
&lt;p&gt;First, note that a &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; variate can be generated via
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1)))\)&lt;/span&gt;. This is a straightforward application of
the
&lt;a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling"&gt;inverse transform sampling&lt;/a&gt;
method for random number generation. This means that if we use the same sequence
of uniform random variates then, &lt;span class="math"&gt;\(z_i = -\log(-\log(u_i))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, this does not give use equality between &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;, but it does
turn out that &lt;span class="math"&gt;\(k_i = -\log(-\log(e_i))\)&lt;/span&gt;, which is useful because this is a
monotonic transformation on the interval &lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;. Since monotonic
transformations preserve ordering, the sequences &lt;span class="math"&gt;\(k\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; result in the same
comparison decisions, as well as, the same argmax. In summary, the algorithms
are the same!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extensions&lt;/strong&gt;: After reading a little further along in the ES paper, we see
that the same algorithm can be used to perform &lt;em&gt;sampling without replacement&lt;/em&gt; by
sorting and taking the elements with the highest keys. This same modification is
applicable to the Gumbel-max-trick because the keys have exactly the same
ordering as ES.In practice we don't sort the key, but instead use a bounded
priority queue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing&lt;/strong&gt;: To the best of my knowledge, the connection between the
Gumbel-max-trick and ES is undocumented. Furthermore, the Gumbel-max-trick is
not known as a streaming algorithm, much less known to perform sampling without
replacement! If you know of a reference let me know. Perhaps, we'll finish
turning these connections into a
&lt;a href="https://github.com/timvieira/gumbel"&gt;short tech report&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Fri, 01 Aug 2014 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2014-08-01:blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</guid><category>math</category><category>sampling</category><category>Gumbel</category></item><item><title>Gumbel max trick</title><link>http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/</link><description>&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Sampling from a discrete distribution parametrized by unnormalized
log-probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_k = \frac{1}{z} \exp(x_k)   \ \ \ \text{where } z = \sum_{j=1}^K \exp(x_j)
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The usual way&lt;/strong&gt;: Exponentiate and normalize (using the
&lt;a href="/blog/post/2014/02/11/exp-normalize-trick/"&gt;exp-normalize trick&lt;/a&gt;), then use the
an algorithm for sampling from a discrete distribution (aka categorical):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;usual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;     &lt;span class="c1"&gt;# the exp-normalize trick&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The Gumbel max trick&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \text{argmax}_{i \in \\{1,\cdots,K\\}} x_i + z_i
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_1 \cdots z_K\)&lt;/span&gt; are i.i.d. &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; random variates. It
turns out that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is distributed according to &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. (See the short derivations
in this
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;blog post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Implementing the Gumbel max trick is remarkable easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gumbel_max_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gumbel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't have access to a Gumbel random variate generator, you can use
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gumbel max requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; samples from a uniform. Usual requires only &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gumbel is one-pass because it does not require normalization (or a pass to
     compute the max for use in the exp-normalize trick). More on this in a
     later post!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Gumbel max trick requires &lt;span class="math"&gt;\(2K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt;, whereas ordinary
     requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log\)&lt;/span&gt; are expensive
     function, we'd like to avoid calling them. What gives? Well, Gumbel's calls
     to &lt;span class="math"&gt;\(\log\)&lt;/span&gt; do not depend on the data so they can be precomputed; this is
     handy for implementations which rely on vectorization for efficiency,
     e.g. python+numpy. By the way, an interesting alternative approach to
     avoiding &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; is described on Justin Domke's blog post on
     &lt;a href="http://justindomke.wordpress.com/2014/01/08/reducing-sigmoid-computations-by-at-least-88-0797077977882/"&gt;speeding up sampling from a sigmoid&lt;/a&gt;,
     which discusses a trick for &lt;span class="math"&gt;\(K=2\)&lt;/span&gt;. (Make sure you read the comments!)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Thu, 31 Jul 2014 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2014-07-31:blog/post/2014/07/31/gumbel-max-trick/</guid><category>math</category><category>sampling</category><category>Gumbel</category></item><item><title>Rant against grid search</title><link>http://timvieira.github.io/blog/post/2014/07/22/rant-against-grid-search/</link><description>&lt;p&gt;Grid search is a simple and intuitive algorithm for optimizing and/or exploring
the effects of parameters to a function. However, given its rigid definition
grid search is susceptible to degenerate behavior. One type of unfortunate
behavior occurs in the presence of unimportant parameters, which results in many
(potentially expensive) function evaluations being wasted.&lt;/p&gt;
&lt;p&gt;This is a very simple point, but nonetheless I'll illustrate with a simple
example.&lt;/p&gt;
&lt;p&gt;Consider the following simple example, let's find the argmax of &lt;span class="math"&gt;\(f(x,y) = -x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we search over a &lt;span class="math"&gt;\(10\)&lt;/span&gt;-by-&lt;span class="math"&gt;\(10\)&lt;/span&gt; grid, resulting in a total of &lt;span class="math"&gt;\(100\)&lt;/span&gt;
function evaluations. For this function, we expect precision which proportional
of the number of samples in the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dimension, which is only &lt;span class="math"&gt;\(10\)&lt;/span&gt; samples! On
the other hand, randomly sampling points over the same space results in &lt;span class="math"&gt;\(100\)&lt;/span&gt;
samples in every dimension.&lt;/p&gt;
&lt;p&gt;In other words, randomly sample instead of using a rigid grid. If you have
points, which are not uniformly spaced, I'm willing to bet that an appropriate
probability distribution exists.&lt;/p&gt;
&lt;p&gt;This type of problem is common on hyperparameter optimizations. For futher
reading see
&lt;a href="http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;Bergstra &amp;amp; Bengio (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other thoughts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Local search is often much more effective. For example, gradient-based
   optimization, Nelder-Mead, stochastic local search, coordinate ascent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grid search tends to produce nicer-looking plots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What about variance in the results? Two things: (a) This is a concern for
   replicability, but is easily remedied by making sampled parameters
   available. (b) There is always some probability that the sampling gives you a
   terrible set of points. This shouldn't be a problem if you use enough
   samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Tue, 22 Jul 2014 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2014-07-22:blog/post/2014/07/22/rant-against-grid-search/</guid><category>misc</category></item><item><title>Expected value of a quadratic and the Delta method</title><link>http://timvieira.github.io/blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</link><description>&lt;p&gt;&lt;strong&gt;Expected value of a quadratic&lt;/strong&gt;: Suppose we'd like to compute the expectation
of a quadratic function, i.e.,
&lt;span class="math"&gt;\(\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]\)&lt;/span&gt; , where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is
a random vector and &lt;span class="math"&gt;\(A\)&lt;/span&gt; is deterministic &lt;em&gt;symmetric&lt;/em&gt; matrix. Let &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; be the mean and variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. It turns out the expected value of a
quadratic has the following simple form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]
=
\text{trace}\left( A \Sigma \right) + \mu^{\top}\negthinspace A \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Delta Method&lt;/strong&gt;: Suppose we'd like to compute expected value of a nonlinear
function &lt;span class="math"&gt;\(f\)&lt;/span&gt; applied our random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\mathbb{E}\left[ f(x) \right]\)&lt;/span&gt;. The Delta method approximates this expection by
replacing &lt;span class="math"&gt;\(f\)&lt;/span&gt; by it's second-order Talylor approximation &lt;span class="math"&gt;\(\hat{f_{a}}\)&lt;/span&gt; taken at
some point &lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{f_{a}}(x) = f(a) + \nabla f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^\top H(a) (x - a)
$$&lt;/div&gt;
&lt;p&gt;The expectation of this Talyor approximation is a quadratic function! Let's try
to apply our new equation for the expected value of quadratic. We can use the
trick from above with &lt;span class="math"&gt;\(A=H(a)\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = (x-a)\)&lt;/span&gt;. Note, covariance matrix is shift
invariant and the Hessian is a symmetric matrix!&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\mathbb{E}\left[ \hat{f_{a}}(x) \right]
 &amp;amp; = \mathbb{E} \left[ f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) + \frac{1}{2} \mathbb{E} \left[ (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) +
   \frac{1}{2}\left( \text{trace}\left( H(a) \, \Sigma \right) + (\mu - a)^{\top} H(a)\, (\mu - a) \right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;It's common to take the Taylor expansion around &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. This simplifies the equation&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\mathbb{E}\left[ \hat{f_{\mu}} (x) \right]
&amp;amp;= \mathbb{E}\left[ f(\mu) + \nabla\negthinspace f(\mu) (x - \mu) + \frac{1}{2} (x - \mu)^{\top} H(\mu)\, (x - \mu) \right] \\\
&amp;amp;= f(\mu) + \frac{1}{2} \, \text{trace}\Big( H(\mu) \, \Sigma \Big)
\end{aligned}&lt;/div&gt;
&lt;p&gt;That looks much more tractable! Error bounds are possible to derive, but outside
to scope of this post. For a nice use of the delta method in machine learning
see &lt;a href="http://arxiv.org/pdf/1307.1493v2.pdf"&gt;(Wager+,'13)&lt;/a&gt; and
&lt;a href="http://cs.jhu.edu/~jason/papers/smith+eisner.acl06-risk.pdf"&gt;(Smith &amp;amp; Eisner,'06)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 21 Jul 2014 00:00:00 -0400</pubDate><guid>tag:timvieira.github.io,2014-07-21:blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</guid><category>math</category></item><item><title>Visualizing high-dimensional functions with cross-sections</title><link>http://timvieira.github.io/blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</link><description>&lt;p&gt;Last September, I gave a talk which included a bunch of two-dimensional plots of
a high-dimensional objective I was developing specialized algorithms for
optimizing. A month later, at least three of my colleagues told me that my plots
had inspired them to make similar plots. The plotting trick is really simple and
not original, but nonetheless I'll still write it up for all to enjoy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example plot&lt;/strong&gt;: This image shows cross-sections of two related functions: a
non-smooth (black) and a smooth approximating function (blue). The plot shows
that the approximation is faithful to the overall shape, but sometimes
over-smooths. In this case, we miss the maximum, which happens near the middle
of the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/blog/images/cross-section.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Let &lt;span class="math"&gt;\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)&lt;/span&gt; be a high-dimensional
function (&lt;span class="math"&gt;\(d \gg 2\)&lt;/span&gt;), which you'd like to visualize. Unfortunately, you are like
me and can't see in high-dimensions what do you do?&lt;/p&gt;
&lt;p&gt;One simple thing to do is take a nonzero vector &lt;span class="math"&gt;\(\boldsymbol{d} \in
\mathbb{R}^d\)&lt;/span&gt;, take a point of interest &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;, and build a local
picture of &lt;span class="math"&gt;\(f\)&lt;/span&gt; by evaluating it at various intervals along the chosen direction
as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$
f_i = f(\boldsymbol{x} + \alpha_i \ \boldsymbol{d}) \ \ \text{for } \alpha_i \in [\alpha_\min, \alpha_\max]
$$&lt;/div&gt;
&lt;p&gt;Of course, you'll have to pick a reasonable range and discretize it. Note,
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; are fixed for all &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;. Now, you can
plot &lt;span class="math"&gt;\((\alpha_i,f_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Picking directions&lt;/strong&gt;: There are many alternatives for picking
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;, my favorites are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gradient (if it exists), this direction is guaranteed to show a local
    increase/decrease in the objective, unless it's zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Coordinate vectors. Varying one dimension per plot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random. I recommend directions drawn from a spherical
    Gaussian.&lt;sup id="fnref:sphericalgaussian"&gt;&lt;a class="footnote-ref" href="#fn:sphericalgaussian" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; The reason being that such a vector is
    uniformly distributed across all unit-length directions (i.e., the angle of
    the vector, not it's length). We will vary the length ourselves via
    &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. It's probably best that our plots don't randomly vary in scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Extension to 3d&lt;/strong&gt;: It's pretty easy to extend this generating 3d plots by
using 2 vectors, &lt;span class="math"&gt;\(\boldsymbol{d_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d_2}\)&lt;/span&gt;, and varying two
parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{x} + \alpha \ \boldsymbol{d_1} + \beta \ \boldsymbol{d_2})
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Closing remarks&lt;/strong&gt;: These types of plots are probably best used to: empirically
verify/explore properties of an objective function, compare approximations, test
sensitivity to certain parameters/hyperparameters, visually debug optimization
algorithms.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:sphericalgaussian"&gt;
&lt;p&gt;More formally, vectors drawn from a spherical Gaussian are
points uniformly distributed on the surface of a &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional unit sphere,
&lt;span class="math"&gt;\(\mathbb{S}^d\)&lt;/span&gt;. Sampling a vector from a spherical Gaussian is straightforward:
sample &lt;span class="math"&gt;\(\boldsymbol{d'} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\boldsymbol{d} = \boldsymbol{d'} / \| \boldsymbol{d'} \|_2\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:sphericalgaussian" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Wed, 12 Feb 2014 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2014-02-12:blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</guid><category>math</category><category>visualization</category></item><item><title>Exp-normalize trick</title><link>http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/</link><description>&lt;p&gt;This trick is the very close cousin of the infamous log-sum-exp trick
(&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html"&gt;scipy.misc.logsumexp&lt;/a&gt;),&lt;/p&gt;
&lt;p&gt;Supposed you'd like to evaluate a probability distribution &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt;
parametrized by a vector &lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}^n\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i = \frac{ \exp(x_i) }{ \sum_{j=1}^n \exp(x_j) }
$$&lt;/div&gt;
&lt;p&gt;The exp-normalize trick leverages the following identity to avoid numerical
overflow. For any &lt;span class="math"&gt;\(b \in \mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i
= \frac{ \exp(x_i - b) \exp(b) }{ \sum_{j=1}^n \exp(x_j - b) \exp(b) }
= \frac{ \exp(x_i - b) }{ \sum_{j=1}^n \exp(x_j - b) }
$$&lt;/div&gt;
&lt;p&gt;In other words, the &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is shift-invariant. A reasonable choice
is &lt;span class="math"&gt;\(b = \max_{i=1}^n x_i\)&lt;/span&gt;. With this choice, overflow due to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; is
impossible&lt;span class="math"&gt;\(-\)&lt;/span&gt;the largest number exponentiated after shifting is &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exp-normalize v. log-sum-exp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If what you want to remain in log-space, that is, compute
&lt;span class="math"&gt;\(\log(\boldsymbol{\pi})\)&lt;/span&gt;, you should use logsumexp. However, if
&lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is your goal, then exp-normalize trick is for you! Since it
avoids additional calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;, which would be required if using log-sum-exp
and more importantly exp-normalize is more numerically stable!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log-sum-exp for computing the log-distibution&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log \pi_i = x_i - \mathrm{logsumexp}(\boldsymbol{x})
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{logsumexp}(\boldsymbol{x}) = b + \log \sum_{j=1}^n \exp(x_j - b)
$$&lt;/div&gt;
&lt;p&gt;Typically with the same choice for &lt;span class="math"&gt;\(b\)&lt;/span&gt; as above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numerically-stable sigmoid function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sigmoid function can be computed with the exp-normalize trick in order to
avoid numerical overflow. In the case of &lt;span class="math"&gt;\(\text{sigmoid}(x)\)&lt;/span&gt;, we have a
distribution with unnormalized log probabilities &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt;, where we are only
interested in the probability of the first event. From the exp-normalize
identity, we know that the distributions &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt; and &lt;span class="math"&gt;\([0,-x]\)&lt;/span&gt; are equivalent (to
see why, plug in &lt;span class="math"&gt;\(b=\max(0,x)\)&lt;/span&gt;). This is why sigmoid is often expressed in one
of two equivalent ways:&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{sigmoid}(x) = 1/(1+\exp(-x)) = \exp(x) / (\exp(x) + 1)
$$&lt;/div&gt;
&lt;p&gt;Interestingly, each version covers an extreme case: &lt;span class="math"&gt;\(x=\infty\)&lt;/span&gt; and &lt;span class="math"&gt;\(x=-\infty\)&lt;/span&gt;,
respectively. Below is some python code which implements the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Numerically-stable sigmoid function.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# if x is less than zero then z will be small, denom can&amp;#39;t be&lt;/span&gt;
        &lt;span class="c1"&gt;# zero because it&amp;#39;s 1+z.&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Tue, 11 Feb 2014 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2014-02-11:blog/post/2014/02/11/exp-normalize-trick/</guid><category>math</category><category>numerical</category></item><item><title>Gradient-vector product</title><link>http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/</link><description>&lt;p&gt;We've all written the following test for our gradient code (known as the
finite-difference approximation).&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial x_i} f(\boldsymbol{x}) \approx
 \frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{e_i})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{e_i})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt; is a vector of zeros except at
&lt;span class="math"&gt;\(i\)&lt;/span&gt; where it is &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This approximation is exact in the limit, and accurate to
&lt;span class="math"&gt;\(o(\varepsilon^2)\)&lt;/span&gt; additive error.&lt;/p&gt;
&lt;p&gt;This is a specific instance of a more general approximation! The dot product of
the gradient and any (conformable) vector &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; can be approximated
with the following formula,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla f(\boldsymbol{x})^{\top} \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;We get the special case above when &lt;span class="math"&gt;\(\boldsymbol{d}=\boldsymbol{e_i}\)&lt;/span&gt;. This also
exact in the limit and just as accurate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Runtime?&lt;/strong&gt; Finite-difference approximation is probably too slow for
  approximating a high-dimensional gradient because the number of function
  evaluations required is &lt;span class="math"&gt;\(2 n\)&lt;/span&gt; where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the dimensionality of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. However,
  if the end goal is to approximate a gradient-vector product, a mere &lt;span class="math"&gt;\(2\)&lt;/span&gt;
  function evaluations is probably faster than specialized code for computing
  the gradient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to set &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;?&lt;/strong&gt; The second approach is more sensitive to
  &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; because &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; is arbitrary, unlike
  &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt;, which is a simple unit-norm vector. Luckily some guidance
  is available. Andrei (2009) reccommends&lt;/p&gt;
&lt;div class="math"&gt;$$
\varepsilon = \sqrt{\epsilon_{\text{mach}}} (1 + \|\boldsymbol{x} \|_{\infty}) / \| \boldsymbol{d} \|_{\infty}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\epsilon_{\text{mach}}\)&lt;/span&gt; is
&lt;a href="http://en.wikipedia.org/wiki/Machine_epsilon"&gt;machine epsilon&lt;/a&gt;. (Numpy users:
&lt;code&gt;numpy.finfo(x.dtype).eps&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Why do I care?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Well, I tend to work on sparse, but high-dimensional problems where
   finite-difference would be too slow. Thus, my usual solution is to only test
   several randomly selected dimensions&lt;span class="math"&gt;\(-\)&lt;/span&gt;biasing samples toward dimensions
   which should be nonzero. With the new trick, I can effectively test more
   dimensions at once by taking random vectors &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;. I recommend
   sampling &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; from a spherical Gaussian so that we're uniform on
   the angle of the vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sometimes the gradient-vector dot product is the end goal. This is the case
   with Hessian-vector products, which arises in many optimization algorithms,
   such as stochastic meta descent. Hessian-vector products are an instance of
   the gradient-vector dot product because the Hessian is just the gradient of
   the gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Hessian-vector product&lt;/h2&gt;
&lt;p&gt;Hessian-vector products are an instance of the gradient-vector dot product
because since the Hessian is just the gradient of the gradient! Now you only
need to remember one formula!&lt;/p&gt;
&lt;div class="math"&gt;$$
H(\boldsymbol{x})\, \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
  \nabla f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
- \nabla f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
\Big)
$$&lt;/div&gt;
&lt;p&gt;With this trick you never have to actually compute the gnarly Hessian! More on
&lt;a href="http://justindomke.wordpress.com/2009/01/17/hessian-vector-products/"&gt;Justin Domke's blog&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = file:////home/timv/projects/blog/output/MathJax.js;
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 10 Feb 2014 00:00:00 -0500</pubDate><guid>tag:timvieira.github.io,2014-02-10:blog/post/2014/02/10/gradient-vector-product/</guid><category>math</category><category>calculus</category></item></channel></rss>