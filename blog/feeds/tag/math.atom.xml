<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Graduate Descent</title><link href="http://timvieira.github.io/blog/" rel="alternate"></link><link href="/blog/feeds/tag/math.atom.xml" rel="self"></link><id>http://timvieira.github.io/blog/</id><updated>2014-07-22T00:00:00-04:00</updated><entry><title>Rant against grid search</title><link href="http://timvieira.github.io/blog/post/2014/07/22/rant-against-grid-search/" rel="alternate"></link><updated>2014-07-22T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io/blog,2014-07-22:post/2014/07/22/rant-against-grid-search/</id><summary type="html">&lt;p&gt;Grid search is a simple and intuitive algorithm for optimizing and/or exploring
the effects of parameters to a function. However, given its rigid definition
grid search is susceptible to degenerate behavior. One type of unfortunate
behavior occurs in the presence of unimportant parameters, which results in many
(potentially expensive) function evaluations being wasted.&lt;/p&gt;
&lt;p&gt;This is a very simple point, but nonetheless I'll illustrate with a simple
example.&lt;/p&gt;
&lt;p&gt;Consider the following simple example, let's find the argmax of $f(x,y) = -x^2$.&lt;/p&gt;
&lt;p&gt;Suppose we search over a $10$-by-$10$ grid, resulting in a total of $100$
function evaluations. For this function, we expect precision which proportional
of the number of samples in the $x$-dimension, which is only $10$ samples! On
the other hand, randomly sampling points over the same space results in $100$
samples in every dimension.&lt;/p&gt;
&lt;p&gt;In other words, randomly sample instead of using a rigid grid. If you have
points, which are not uniformly spaced, I'm willing to bet that an appropriate
probability distribution exists.&lt;/p&gt;
&lt;p&gt;This type of problem is common on hyperparameter optimizations. For futher
reading see
&lt;a href="http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;Bergstra &amp;amp; Bengio (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other thoughts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Local search is often much more effective. For example, gradient-based
   optimization, Nelder-Mead, stochastic local search, coordinate ascent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grid search tends to produce nicer-looking plots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What about variance in the results? Two things: (a) This is a concern for
   replicability, but is easily remedied by making sampled parameters
   available. (b) There is always some probability that the sampling gives you a
   terrible set of points. This shouldn't be a problem if you use enough
   samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="math"></category></entry><entry><title>Expected value of a quadratic and the Delta method</title><link href="http://timvieira.github.io/blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/" rel="alternate"></link><updated>2014-07-21T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io/blog,2014-07-21:post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Expected value of a quadratic&lt;/strong&gt;: Suppose we'd like to compute the expectation
of a quadratic function, i.e.,
$\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]$ , where $x$ is
a random vector and $A$ is deterministic &lt;em&gt;symmetric&lt;/em&gt; matrix. Let $\mu$ and
$\Sigma$ be the mean and variance of $x$. It turns out the expected value of a
quadratic has the following simple form:&lt;/p&gt;
&lt;p&gt;$$
\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]
=
\text{trace}\left( A \Sigma \right) + \mu^{\top}\negthinspace A \mu
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Delta Method&lt;/strong&gt;: Suppose we'd like to compute expected value of a nonlinear
function $f$ applied our random variable $x$,
$\mathbb{E}\left[ f(x) \right]$. The Delta method approximates this expection by
replacing $f$ by it's second-order Talylor approximation $\hat{f_{a}}$ taken at
some point $a$&lt;/p&gt;
&lt;p&gt;$$
\hat{f_{a}}(x) = f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^\top H(a)\, (x - a)
$$&lt;/p&gt;
&lt;p&gt;The expectation of this Talyor approximation is a quadratic function! Let's try
to apply our new equation for the expected value of quadratic. We can use the
trick from above with $A=H(a)$ and $x = (x-a)$. Note, covariance matrix is shift
invariant and the Hessian is a symmetric matrix!&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb{E}\left[ \hat{f_{a}}(x) \right]
 &amp;amp; = \mathbb{E} \left[ f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^{\top} H(a)\, (x - a) \right] \\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) + \frac{1}{2} \mathbb{E} \left[ (x - a)^{\top} H(a)\, (x - a) \right] \\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) +
   \frac{1}{2}\left( \text{trace}\left( H(a) \, \Sigma \right) + (\mu - a)^{\top} H(a)\, (\mu - a) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;It's common to take the Taylor expansion around $\mu$. This simplifies the equation&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\mathbb{E}\left[ \hat{f_{\mu}} (x) \right]
&amp;amp;= \mathbb{E}\left[ f(\mu) + \nabla\negthinspace f(\mu) (x - \mu) + \frac{1}{2} (x - \mu)^{\top} H(\mu)\, (x - \mu) \right] \\
&amp;amp;= f(\mu) + \frac{1}{2} \, \text{trace}\Big( H(\mu) \, \Sigma \Big)
\end{aligned}&lt;/p&gt;
&lt;p&gt;That looks much more tractable! Error bounds are possible to derive, but outside
to scope of this post. For a nice use of the delta method in machine learning
see &lt;a href="http://arxiv.org/pdf/1307.1493v2.pdf"&gt;(Wager+,'13)&lt;/a&gt; and
&lt;a href="http://cs.jhu.edu/~jason/papers/smith+eisner.acl06-risk.pdf"&gt;(Smith &amp;amp; Eisner,'06)&lt;/a&gt;&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="math"></category></entry></feed>