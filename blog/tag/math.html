<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Graduate Descent - math</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />


  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="../favicon.png" rel="icon">
  <link href="../theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="../theme/js/modernizr-2.0.js"></script>
  <script src="../theme/js/ender.js"></script>
  <script src="../theme/js/octopress.js" type="text/javascript"></script>

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="../">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>



<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/archives.html">Archive</a></li>
    <li >
    <a href="../category/misc.html">Misc</a>
    </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div class="blog-index">
  		<article>
<header>
      <h1 class="entry-title">
        <a href="../post/2016/05/27/dimensional-analysis-of-gradient-ascent/">Dimensional analysis of gradient ascent</a>
      </h1>
      <p class="meta"><time datetime="2016-05-27T00:00:00-04:00" pubdate>May 27, 2016</time></p>
</header>

  <div class="entry-content"><p>In physical sciences, numbers are paired with units and called quantities. In
this augmented number system, dimensional analysis provides a crucial sanity
check, much like type checking in a programming language. There are simple rules
for building up units and constraints on what operations are allowed. For
example, you can't multiply quantities which are not conformable or add
quantities with different units. Also, we generally know the units of the input
and desired output, which allows us to check that our computations at least
produce the right units.</p>
<p>In this post, we'll discuss the dimensional analysis of gradient ascent, which
will hopefully help us understand why the "step size" is parameter so finicky
and why it even exists.</p>
<p>Gradient ascent is an iterative procedure for (locally) maximizing a function,
<span class="math">\(f: \mathbb{R}^d \mapsto \mathbb{R}\)</span>.</p>
<div class="math">$$
x_{t+1} = x_t + \alpha \frac{\partial f(x_t)}{\partial x}
$$</div>
<p>In general, <span class="math">\(\alpha\)</span> is a <span class="math">\(d \times d\)</span> matrix, but often we constrain the matrix
to be simple, e.g., <span class="math">\(a\cdot I\)</span> for some scalar <span class="math">\(a\)</span> or <span class="math">\(\text{diag}(a)\)</span> for some
vector <span class="math">\(a\)</span>.</p>
<p>Now, let's look at the units of the change in <span class="math">\(\Delta x=x_{t+1} - x_t\)</span>,
</p>
<div class="math">$$
(\textbf{units }\Delta x) = \left(\textbf{units }\alpha\cdot \frac{\partial f(x_t)}{\partial x}\right) = (\textbf{units }\alpha) \frac{(\textbf{units }f)}{(\textbf{units }x)}.
$$</div>
<p>The units of <span class="math">\(\Delta x\)</span> must be <span class="math">\((\textbf{units }x)\)</span>. However, if we assume <span class="math">\(f\)</span>
is unit free, we're happy with <span class="math">\((\textbf{units }x) / (\textbf{units }f)\)</span>.</p>
<p>Solving for the units of <span class="math">\(\alpha\)</span> we get,
</p>
<div class="math">$$
(\textbf{units }\alpha) = \frac{(\textbf{units }x)^2}{(\textbf{units }f)}.
$$</div>
<p>This gives us an idea for what <span class="math">\(\alpha\)</span> should be.</p>
<p>For example, the inverse Hessian passes the unit check (if we assume <span class="math">\(f\)</span> unit
free). The disadvantages of the Hessian is that it needs to be positive-definite
(or at least invertible) in order to be a valid "step size" (i.e., we need
step sizes to be <span class="math">\(&gt; 0\)</span>).</p>
<p>Another method for handling step sizes is line search. However, line search
won't let us run online. Furthermore, line search would be too slow in the case
where we want a step size for each dimension.</p>
<p>In machine learning, we've become fond of online methods, which adapt the step
size as they go. The general idea is to estimate a step size matrix that passes
the unit check (for each dimension of <span class="math">\(x\)</span>). Furthermore, we want do as little
extra work as possible to get this estimate (e.g., we want to avoid computing a
Hessian because that would be extra work). So, the step size should be based
only iterates and gradients up to time <span class="math">\(t\)</span>.</p>
<ul>
<li>
<p><a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">AdaGrad</a> doesn't doesn't
  pass the unit check. This motivated AdaDelta.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1212.5701">AdaDelta</a> uses the ratio of (running
  estimates of) the root-mean-squares of <span class="math">\(\Delta x\)</span> and <span class="math">\(\partial f / \partial
  x\)</span>. The mean is taken using an exponentially weighted moving average. See
  paper for actual implementation.</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1412.6980">Adam</a> came later and made some tweaks to
  remove (unintended) bias in the AdaDelta estimates of the numerator and
  denominator.</p>
</li>
</ul>
<p>In summary, it's important/useful to analyze the units of numerical algorithms
in order to get a sanity check (i.e., catch mistakes) as well as to develop an
understanding of why certain parameters exist and how properties of a problem
affect the values we should use for them.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
  <footer>
    <a rel="full-article" href="../post/2016/05/27/dimensional-analysis-of-gradient-ascent/">Read On &crarr;</a>
  </footer>
  		</article>
  		<article>
<header>
      <h1 class="entry-title">
        <a href="../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/">Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</a>
      </h1>
      <p class="meta"><time datetime="2016-03-05T00:00:00-05:00" pubdate>Mar 05, 2016</time></p>
</header>

  <div class="entry-content"><p>The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization---the "inner" optimization optimizes training loss (wrt <span class="math">\(\theta\)</span>),
while the "outer" optimizes hyperparameters (<span class="math">\(\lambda\)</span>).</p>
<div class="math">$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$</div>
<p>Can we estimate <span class="math">\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)</span> so
that we can run gradient-based optimization over <span class="math">\(\lambda\)</span>?</p>
<p>Well, what does it mean to have an <span class="math">\(\textbf{argmin}\)</span> inside a function?</p>
<p>Well, it means that there is a <span class="math">\(\theta^*\)</span> that gets passed to
<span class="math">\(\mathcal{L}_{\text{dev}}\)</span>. And, <span class="math">\(\theta^*\)</span> is a function of <span class="math">\(\lambda\)</span>, denoted
<span class="math">\(\theta(\lambda)\)</span>. Furthermore, <span class="math">\(\textbf{argmin}\)</span> must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as</p>
<div class="math">$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$</div>
<p>
where <span class="math">\(\theta(\lambda)\)</span> is the solution to,
</p>
<div class="math">$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$</div>
<p>Now how does <span class="math">\(\theta\)</span> change as the result of an infinitesimal change to
<span class="math">\(\lambda\)</span>?</p>
<p>The constraint on the derivative implies a type of "equilibrium"---the inner
optimization process will continue to optimize regardless of how we change
<span class="math">\(\lambda\)</span>. Assuming we don't change <span class="math">\(\lambda\)</span> too much, then the inner
optimization shouldn't change <span class="math">\(\theta\)</span> too much and it will change in a
predictable way.</p>
<p>To do this, we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are related through a
function <span class="math">\(g\)</span> as follows,</p>
<div class="math">$$g(x,y) = 0.$$</div>
<p>Assuming <span class="math">\(g\)</span> is a smooth function in <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, we can perturb either
argument, say <span class="math">\(x\)</span> by a small amount <span class="math">\(\Delta_x\)</span> and <span class="math">\(y\)</span> by <span class="math">\(\Delta_y\)</span>. Because
system preserves the constraint, i.e.,</p>
<div class="math">$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$</div>
<p>We can solve for the change of <span class="math">\(x\)</span> as a result of an infinitesimal change in
<span class="math">\(y\)</span>. We take the first-order expansion,</p>
<div class="math">$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$</div>
<p>Since <span class="math">\(g(x,y)\)</span> is already zero,</p>
<div class="math">$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$</div>
<p>Next, we solve for <span class="math">\(\frac{\Delta_x}{\Delta_y}\)</span>.</p>
<div class="math">$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$</div>
<div class="math">$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$</div>
<p>Back to the original problem: Now we can use the implicit function theorem to
estimate how <span class="math">\(\theta\)</span> varies in <span class="math">\(\lambda\)</span> by plugging in <span class="math">\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)</span>, <span class="math">\(x \mapsto \theta\)</span>
and <span class="math">\(y \mapsto \lambda\)</span>:</p>
<div class="math">$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$</div>
<p>This tells us how <span class="math">\(\theta\)</span> changes with respect to an infinitesimal change to
<span class="math">\(\lambda\)</span>. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt <span class="math">\(\lambda\)</span>,</p>
<div class="math">$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$</div>
<p>Since we don't like (explicit) matrix inverses, we compute <span class="math">\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)</span> as the solution to <span class="math">\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)</span>. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products---i.e., you never have to materialize the Hessian. (Apparently,
<a href="https://en.wikipedia.org/wiki/Matrix-free_methods">matrix-free linear algebra</a>
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
<a href="/blog/post/2014/02/10/gradient-vector-product/">earlier post</a>).</p>
<p>At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.</p>
<p>In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.</p>
<p><strong>Further reading</strong>:</p>
<ul>
<li>
<p><a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/">Truncated Bi-Level Optimization</a></p>
</li>
<li>
<p><a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf">Efficient multiple hyperparameter learning for log-linear models</a></p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1502.03492">Gradient-based Hyperparameter Optimization through Reversible Learning</a></p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
  <footer>
    <a rel="full-article" href="../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/">Read On &crarr;</a>
  </footer>
  		</article>
  		<article>
<header>
      <h1 class="entry-title">
        <a href="../post/2015/07/29/gradient-of-a-product/">Gradient of a product</a>
      </h1>
      <p class="meta"><time datetime="2015-07-29T00:00:00-04:00" pubdate>Jul 29, 2015</time></p>
</header>

  <div class="entry-content"><div class="math">$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$</div>
<p>In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).</p>
<p>Given a collection of functions with a common argument <span class="math">\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)</span>.</p>
<p>Define their product <span class="math">\(p(x) = \prod_{i=1}^n f_i(x)\)</span></p>
<p>Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, <span class="math">\(x\)</span>.</p>
<div class="math">$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;=&amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;=&amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$</div>
<p>As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."</p>
<p>A naive method for computing the gradient computes the leave-one-out products
from scratch for each <span class="math">\(i\)</span> (outer loop)---resulting in a overall runtime of
<span class="math">\(O(n^2)\)</span> to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.</p>
<p><strong>Division trick</strong>: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:</p>
<div class="math">$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;=&amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;=&amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$</div>
<p>Pro:</p>
<ul>
<li>Runtime <span class="math">\(\bigo(n)\)</span> with space <span class="math">\(\bigo(1)\)</span>.</li>
</ul>
<p>Con:</p>
<ul>
<li>
<p>Requires <span class="math">\(f \ne 0\)</span>. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.</p>
</li>
<li>
<p>Requires multiplicative inverse operator (division) <em>and</em>
   associative-commutative multiplication, which means it's not applicable to
   matrices.</p>
</li>
</ul>
<p><strong>Log trick</strong>: Suppose <span class="math">\(f_i\)</span> are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large <span class="math">\(n\)</span>). It's common practice (especially in machine learning) to replace
<span class="math">\(f_i\)</span> with <span class="math">\(\log f_i\)</span>, which turns products into sums, <span class="math">\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)</span>, and tiny numbers (like
<span class="math">\(\texttt{3.72e-44}\)</span>) into large ones (like <span class="math">\(\texttt{-100}\)</span>).</p>
<p>Furthermore, using the identity <span class="math">\((\nabla g) = g \cdot \nabla \log g\)</span>, we can
operate exclusively in the "<span class="math">\(\log\)</span>-domain".</p>
<div class="math">$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;=&amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$</div>
<p>Pro:</p>
<ul>
<li>
<p>Numerically stable</p>
</li>
<li>
<p>Runtime <span class="math">\(\bigo(n)\)</span> with space <span class="math">\(\bigo(1)\)</span>.</p>
</li>
<li>
<p>Doesn't require multiplicative inverse assuming you can compute <span class="math">\(\gradx{ \log
   f_i(x) }\)</span> without it.</p>
</li>
</ul>
<p>Con:</p>
<ul>
<li>
<p>Requires <span class="math">\(f &gt; 0\)</span>. But, we can use
   <a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/">LogReal number class</a>
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).</p>
</li>
<li>
<p>Doesn't easily generalize to other notions of multiplication.</p>
</li>
</ul>
<p><strong>Dynamic programming trick</strong>: <span class="math">\(\bigo(n)\)</span> runtime and <span class="math">\(\bigo(n)\)</span> space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. <a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf">Wallach (2004)</a>,
section 7).</p>
<p>The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.</p>
<p>Here are the equations:</p>
<div class="math">$$
\begin{eqnarray*}
\alpha_0(x) &amp;=&amp; 1 \\
\alpha_t(x)
   &amp;=&amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;=&amp; 1 \\
\beta_t(x)
  &amp;=&amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;=&amp; \sum_{i=1}^n \left( \prod_{j &lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &gt; i} f_j(x) \right) \\
&amp;=&amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$</div>
<p>Clearly, this requires <span class="math">\(O(n)\)</span> additional space.</p>
<p>Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).</p>
<p>Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where <span class="math">\(A B C \ne B C A\)</span>, even if all matrices have the
conformable dimensions.</p>
<p><strong>Connections to automatic differentiation</strong>: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
<em>can</em> compute it's gradient with the same asymptotic complexity, <em>but</em> you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with <span class="math">\(\bigo(n^2)\)</span> time and <span class="math">\(\bigo(1)\)</span> space
(other than the space to store the <span class="math">\(n\)</span> functions) and ended up with a <span class="math">\(\bigo(n)\)</span>
time <span class="math">\(\bigo(n)\)</span> space algorithm with a little clever thinking. What I'm saying
is autodiff---even if you don't use a magical package---tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).</p>
<p><em>Sketch</em>: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (<span class="math">\(\alpha\)</span>). It turns out that the backward pass gives us <span class="math">\(\beta\)</span> as
adjoints. Lastly, we'd propagate gradients through the <span class="math">\(f\)</span>'s to get
<span class="math">\(\frac{\partial p}{\partial x}\)</span>. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
  <footer>
    <a rel="full-article" href="../post/2015/07/29/gradient-of-a-product/">Read On &crarr;</a>
  </footer>
  		</article>
<div class="pagination">
    <a class="prev" href="../tag/math2.html">&larr; Older</a>

  <br />
</div></div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="../post/2016/05/27/dimensional-analysis-of-gradient-ascent/">Dimensional analysis of gradient ascent</a>
      </li>
      <li class="post">
          <a href="../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/">Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</a>
      </li>
      <li class="post">
          <a href="../post/2015/07/29/gradient-of-a-product/">Gradient of a product</a>
      </li>
      <li class="post">
          <a href="../post/2015/02/01/log-real-number-class/">Log-Real number class</a>
      </li>
      <li class="post">
          <a href="../post/2014/12/21/importance-sampling/">Importance Sampling</a>
      </li>
    </ul>
  </section>

<!--      
  <section>
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="../category/misc.html">misc</a></li>
    </ul>
  </section>
 
-->

  <section>
  <h1>Tags</h1>
    <a href="../tag/calculus.html">calculus</a>,    <a href="../tag/visualization.html">visualization</a>,    <a href="../tag/statistics.html">statistics</a>,    <a href="../tag/randomized.html">randomized</a>,    <a href="../tag/structured-prediction.html">structured-prediction</a>,    <a href="../tag/numerical.html">numerical</a>,    <a href="../tag/misc.html">misc</a>,    <a href="../tag/sampling.html">sampling</a>,    <a href="../tag/deep-learning.html">deep-learning</a>,    <a href="../tag/rant.html">rant</a>,    <a href="../tag/optimization.html">optimization</a>,    <a href="../tag/crf.html">crf</a>,    <a href="../tag/machine-learning.html">machine-learning</a>,    <a href="../tag/gumbel.html">Gumbel</a>,    <a href="../tag/math.html">math</a>  </section>



  <section>
    <h1>GitHub Repos</h1>
    <ul id="gh_repos">
      <li class="loading">Status updating...</li>
    </ul>
      <a href="https://github.com/timvieira">@timvieira</a> on GitHub
    <script type="text/javascript">
      $.domReady(function(){
          if (!window.jXHR){
              var jxhr = document.createElement('script');
              jxhr.type = 'text/javascript';
              jxhr.src = '../theme/js/jXHR.js';
              var s = document.getElementsByTagName('script')[0];
              s.parentNode.insertBefore(jxhr, s);
          }

          github.showRepos({
              user: 'timvieira',
              count: 4,
              skip_forks: true,
              target: '#gh_repos'
          });
      });
    </script>
    <script src="../theme/js/github.js" type="text/javascript"> </script>
  </section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      var widgetId = '551816176788328448',
          domId = 'tweets',
          count = 3,
          hyperlinkUrlsPlus = true,
          showUserPhoto = false,
          timeOfTweet = true,
          dateFormat = 'default',
          showRetweets = false,
          customOutputFn = null,
          showReplyRetweetButtons = false;

      twitterFetcher.fetch(
        widgetId,
        domId,
        count,
        hyperlinkUrlsPlus,
        showUserPhoto,
        timeOfTweet,
        dateFormat,
        showRetweets,
        customOutputFn,
        showReplyRetweetButtons
      );
    });
  </script>
  <script src="/theme/js/twitter.js" type="text/javascript"> </script>
    <a href="http://twitter.com/xtimv" class="twitter-follow-button" data-show-count="true">Follow @xtimv</a>
</section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014-2016  - Tim Vieira -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>
</body>
</html>