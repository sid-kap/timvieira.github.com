<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Graduate Descent</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />


  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="./favicon.png" rel="icon">
  <link href="./theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="./theme/js/modernizr-2.0.js"></script>
  <script src="./theme/js/ender.js"></script>
  <script src="./theme/js/octopress.js" type="text/javascript"></script>

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="./">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>



<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/archives.html">Archive</a></li>
    <li >
    <a href="./category/misc.html">Misc</a>
    </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div class="blog-index">
  		<article>
<header>
      <h1 class="entry-title">
        <a href="./post/2015/07/29/gradient-of-a-product/">Gradient of a product</a>
      </h1>
      <p class="meta"><time datetime="2015-07-29T00:00:00-04:00" pubdate>Jul 29, 2015</time></p>
</header>

  <div class="entry-content"><div class="math">$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$</div>
<p>In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).</p>
<p>Given a collection of functions with a common argument <span class="math">\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)</span>.</p>
<p>Define their product <span class="math">\(p(x) = \prod_{i=1}^n f_i(x)\)</span></p>
<p>Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, <span class="math">\(x\)</span>.</p>
<div class="math">$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;=&amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;=&amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$</div>
<p>As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."</p>
<p>A naive method for computing the gradient computes the leave-one-out products
from scratch for each <span class="math">\(i\)</span> (outer loop)---resulting in a overall runtime of
<span class="math">\(O(n^2)\)</span> to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.</p>
<p><strong>Division trick</strong>: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:</p>
<div class="math">$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;=&amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;=&amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$</div>
<p>Pro:</p>
<ul>
<li>Runtime <span class="math">\(\bigo(n)\)</span> with space <span class="math">\(\bigo(1)\)</span>.</li>
</ul>
<p>Con:</p>
<ul>
<li>
<p>Requires <span class="math">\(f \ne 0\)</span>. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.</p>
</li>
<li>
<p>Requires multiplicative inverse operator (division) <em>and</em>
   associative-commutative multiplication, which means it's not applicable to
   matrices.</p>
</li>
</ul>
<p><strong>Log trick</strong>: Suppose <span class="math">\(f_i\)</span> are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large <span class="math">\(n\)</span>). It's common practice (especially in machine learning) to replace
<span class="math">\(f_i\)</span> with <span class="math">\(\log f_i\)</span>, which turns products into sums, <span class="math">\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)</span>, and tiny numbers (like
<span class="math">\(\texttt{3.72e-44}\)</span>) into large ones (like <span class="math">\(\texttt{-100}\)</span>).</p>
<p>Furthermore, using the identity <span class="math">\((\nabla g) = g \cdot \nabla \log g\)</span>, we can
operate exclusively in the "<span class="math">\(\log\)</span>-domain".</p>
<div class="math">$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;=&amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$</div>
<p>Pro:</p>
<ul>
<li>
<p>Numerically stable</p>
</li>
<li>
<p>Runtime <span class="math">\(\bigo(n)\)</span> with space <span class="math">\(\bigo(1)\)</span>.</p>
</li>
<li>
<p>Doesn't require multiplicative inverse assuming you can compute <span class="math">\(\gradx{ \log
   f_i(x) }\)</span> without it.</p>
</li>
</ul>
<p>Con:</p>
<ul>
<li>
<p>Requires <span class="math">\(f &gt; 0\)</span>. But, we can use
   <a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/">LogReal number class</a>
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).</p>
</li>
<li>
<p>Doesn't easily generalize to other notions of multiplication.</p>
</li>
</ul>
<p><strong>Dynamic programming trick</strong>: <span class="math">\(\bigo(n)\)</span> runtime and <span class="math">\(\bigo(n)\)</span> space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. <a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf">Wallach (2004)</a>,
section 7).</p>
<p>The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.</p>
<p>Here are the equations:</p>
<div class="math">$$
\begin{eqnarray*}
\alpha_0(x) &amp;=&amp; 1 \\
\alpha_t(x)
   &amp;=&amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;=&amp; 1 \\
\beta_t(x)
  &amp;=&amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;=&amp; \sum_{i=1}^n \left( \prod_{j &lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &gt; i} f_j(x) \right) \\
&amp;=&amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$</div>
<p>Clearly, this requires <span class="math">\(O(n)\)</span> additional space.</p>
<p>Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).</p>
<p>Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where <span class="math">\(A B C \ne B C A\)</span>, even if all matrices have the
conformable dimensions.</p>
<p><strong>Connections to automatic differentiation</strong>: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
<em>can</em> compute it's gradient with the same asymptotic complexity, <em>but</em> you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with <span class="math">\(\bigo(n^2)\)</span> time and <span class="math">\(\bigo(1)\)</span> space
(other than the space to store the <span class="math">\(n\)</span> functions) and ended up with a <span class="math">\(\bigo(n)\)</span>
time <span class="math">\(\bigo(n)\)</span> space algorithm with a little clever thinking. What I'm saying
is autodiff---even if you don't use a magical package---tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).</p>
<p><em>Sketch</em>: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (<span class="math">\(\alpha\)</span>). It turns out that the backward pass gives us <span class="math">\(\beta\)</span> as
adjoints. Lastly, we'd propagate gradients through the <span class="math">\(f\)</span>'s to get
<span class="math">\(\frac{\partial p}{\partial x}\)</span>. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
  <footer>
    <a rel="full-article" href="./post/2015/07/29/gradient-of-a-product/">Read On &crarr;</a>
  </footer>
  		</article>
  		<article>
<header>
      <h1 class="entry-title">
        <a href="./post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/">Multiclass logistic regression and conditional random fields are the same thing</a>
      </h1>
      <p class="meta"><time datetime="2015-04-29T00:00:00-04:00" pubdate>Apr 29, 2015</time></p>
</header>

  <div class="entry-content"><p>A short rant: multiclass logistic regression and conditional random fields (CRF)
are the same thing. This comes to a surprise to many people because CRFs tend to
be surrounded by additional "stuff."</p>
<p>Multiclass logistic regression is simple. The goal is to predict the correct
label <span class="math">\(y^*\)</span> from handful of labels <span class="math">\(\mathcal{Y}\)</span> given the observation <span class="math">\(x\)</span> based
on features <span class="math">\(\phi(x,y)\)</span>. Training this model typically requires computing the
gradient:</p>
<div class="math">$$
\phi(x,y^*) - \sum_{y \in \mathcal{Y}} p(y|x) \phi(x,y)
$$</div>
<p>where
</p>
<div class="math">$$
\begin{eqnarray*}
p(y|x) &amp;=&amp; \frac{1}{Z(x)} \exp(\theta^\top \phi(x,y)) &amp; \ \ \ \ \text{and} \ \ \ \ &amp;
Z(x) &amp;=&amp; \sum_{y \in \mathcal{Y}} \exp(\theta^\top \phi(x,y))
\end{eqnarray*}
$$</div>
<p>At test-time, we often take the highest-scoring label under the model.</p>
<div class="math">$$
\hat{y}(x) = \textbf{argmax}_{y \in \mathcal{Y}} \theta^\top \phi(x,y)
$$</div>
<p>A conditional random field is exactly multiclass logistic regression. The only
difference is that the sum and argmax are inefficient to compute naively (i.e.,
by enumeration). This point is often lost when people first learn about
CRFs. Some people never make this connection.</p>
<p>Here's some stuff you'll see once we start talking about CRFs:</p>
<ol>
<li>
<p>Inference algorithms (e.g., Viterbi decoding, forward-backward, Junction
   tree)</p>
</li>
<li>
<p>Graphical models (factor graphs, Bayes nets, Markov random fields)</p>
</li>
<li>
<p>Model templates (i.e., repeated feature functions)</p>
</li>
</ol>
<p>In the logistic regression case, we'd never use the term "inference" to describe
the "sum" and "max" over a handful of categories. Once we move to a structured
label space, this term gets throw around. (BTW, this isn't "statistical
inference," just algorithms to compute sum and max over <span class="math">\(\mathcal{Y}\)</span>.)</p>
<p>Graphical models establish a notation and structural properties which allow
efficient inference -- things like cycles and treewidth.</p>
<p>Model templating is the only essential trick to move from logistic regression to
a CRF. Templating "solves" the problem that not all training examples have the
same "size" -- the set of outputs <span class="math">\(\mathcal{Y}(x)\)</span> now depends on <span class="math">\(x\)</span>. A model
template specifies how to compute the features for an entire output, by looking
at interactions between subsets of variables.</p>
<div class="math">$$
\phi(x,\boldsymbol{y}) = \sum_{\alpha \in A(x)} \phi_\alpha(x,
\boldsymbol{y}_\alpha)
$$</div>
<p>where <span class="math">\(\alpha\)</span> is a labeled subset of variables often called a factor and
<span class="math">\(\boldsymbol{y}_\alpha\)</span> is the subvector containing values of variables
<span class="math">\(\alpha\)</span>. Basically, the feature function <span class="math">\(\phi\)</span> gets to look at some subset of
the variables being predicted <span class="math">\(y\)</span> and the entire input <span class="math">\(x\)</span>. The ability to look
at more of <span class="math">\(y\)</span> allows the model to make more coherent predictions.</p>
<p>Anywho, it's often useful to take a step back and think about what you are
trying to compute instead of how you're computing it. In this post, this allowed
us see the similarity between logistic regression and CRFs even though they seem
quite different.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
  <footer>
    <a rel="full-article" href="./post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/">Read On &crarr;</a>
  </footer>
  		</article>
  		<article>
<header>
      <h1 class="entry-title">
        <a href="./post/2015/02/05/conditional-random-fields-as-deep-learning-models/">Conditional random fields as Deep learning models?</a>
      </h1>
      <p class="meta"><time datetime="2015-02-05T00:00:00-05:00" pubdate>Feb 05, 2015</time></p>
</header>

  <div class="entry-content"><p>This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.</p>
<p>In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:</p>
<ol>
<li>
<p>Limit the expressiveness of the model (i.e., don't use to model you want)</p>
</li>
<li>
<p>Change the training objective</p>
</li>
<li>
<p>Approximate inference</p>
</li>
</ol>
<p>Approximate inference is tricky. Things can easily go awry.</p>
<p>For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
<a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf">(Kulesza &amp; Pereira, 2007)</a>. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
<a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf">sometimes</a> these loopy BP
approximations works fine.)</p>
<p>It looks like using approximate inference during training changes the training
objective.</p>
<p>So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.</p>
<p>Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp; Eisner.</p>
<p>Unsuprisingly, it works pretty well.</p>
<p>I first saw this idea in Stoyanov &amp; Eisner (2011). They use loopy belief
propagation as their approximate inference algorithm. At the end of the day,
their model is essentially a deep recurrent network, which came from unrolling
inference in a graphical model. This idea really struck me because it's clearly
right in the middle between graphical models and deep learning.</p>
<p>You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.</p>
<p>Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.</p></div>
  		</article>
<div class="pagination">
    <a class="prev" href="./index3.html">&larr; Older</a>

    <a class="next" href="./index.html">Newer &rarr;</a>
  <br />
</div></div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="./post/2016/05/27/dimensional-analysis-of-gradient-ascent/">Dimensional analysis of gradient ascent</a>
      </li>
      <li class="post">
          <a href="./post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/">Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</a>
      </li>
      <li class="post">
          <a href="./post/2016/01/17/multidimensional-array-index/">Multidimensional array index</a>
      </li>
      <li class="post">
          <a href="./post/2015/07/29/gradient-of-a-product/">Gradient of a product</a>
      </li>
      <li class="post">
          <a href="./post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/">Multiclass logistic regression and conditional random fields are the same thing</a>
      </li>
    </ul>
  </section>

<!--      
  <section>
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="./category/misc.html">misc</a></li>
    </ul>
  </section>
 
-->

  <section>
  <h1>Tags</h1>
    <a href="./tag/calculus.html">calculus</a>,    <a href="./tag/visualization.html">visualization</a>,    <a href="./tag/statistics.html">statistics</a>,    <a href="./tag/randomized.html">randomized</a>,    <a href="./tag/structured-prediction.html">structured-prediction</a>,    <a href="./tag/numerical.html">numerical</a>,    <a href="./tag/misc.html">misc</a>,    <a href="./tag/sampling.html">sampling</a>,    <a href="./tag/deep-learning.html">deep-learning</a>,    <a href="./tag/rant.html">rant</a>,    <a href="./tag/optimization.html">optimization</a>,    <a href="./tag/crf.html">crf</a>,    <a href="./tag/machine-learning.html">machine-learning</a>,    <a href="./tag/gumbel.html">Gumbel</a>,    <a href="./tag/math.html">math</a>  </section>



  <section>
    <h1>GitHub Repos</h1>
    <ul id="gh_repos">
      <li class="loading">Status updating...</li>
    </ul>
      <a href="https://github.com/timvieira">@timvieira</a> on GitHub
    <script type="text/javascript">
      $.domReady(function(){
          if (!window.jXHR){
              var jxhr = document.createElement('script');
              jxhr.type = 'text/javascript';
              jxhr.src = './theme/js/jXHR.js';
              var s = document.getElementsByTagName('script')[0];
              s.parentNode.insertBefore(jxhr, s);
          }

          github.showRepos({
              user: 'timvieira',
              count: 4,
              skip_forks: true,
              target: '#gh_repos'
          });
      });
    </script>
    <script src="./theme/js/github.js" type="text/javascript"> </script>
  </section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      var widgetId = '551816176788328448',
          domId = 'tweets',
          count = 3,
          hyperlinkUrlsPlus = true,
          showUserPhoto = false,
          timeOfTweet = true,
          dateFormat = 'default',
          showRetweets = false,
          customOutputFn = null,
          showReplyRetweetButtons = false;

      twitterFetcher.fetch(
        widgetId,
        domId,
        count,
        hyperlinkUrlsPlus,
        showUserPhoto,
        timeOfTweet,
        dateFormat,
        showRetweets,
        customOutputFn,
        showReplyRetweetButtons
      );
    });
  </script>
  <script src="/theme/js/twitter.js" type="text/javascript"> </script>
    <a href="http://twitter.com/xtimv" class="twitter-follow-button" data-show-count="true">Follow @xtimv</a>
</section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014-2016  - Tim Vieira -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>
</body>
</html>