<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />


  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="../../../../../favicon.png" rel="icon">
  <link href="../../../../../theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="../../../../../theme/js/modernizr-2.0.js"></script>
  <script src="../../../../../theme/js/ender.js"></script>
  <script src="../../../../../theme/js/octopress.js" type="text/javascript"></script>

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="../../../../../">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>



<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/archives.html">Archive</a></li>
    <li class="active">
    <a href="../../../../../category/misc.html">Misc</a>
    </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</h1>
      <p class="meta"><time datetime="2016-03-05T00:00:00-05:00" pubdate>Mar 05, 2016</time></p>
</header>

  <div class="entry-content"><p>The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization---the "inner" optimization optimizes training loss (wrt <span class="math">\(\theta\)</span>),
while the "outer" optimizes hyperparameters (<span class="math">\(\lambda\)</span>).</p>
<div class="math">$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$</div>
<p>Can we estimate <span class="math">\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)</span> so
that we can run gradient-based optimization over <span class="math">\(\lambda\)</span>?</p>
<p>Well, what does it mean to have an <span class="math">\(\textbf{argmin}\)</span> inside a function?</p>
<p>Well, it means that there is a <span class="math">\(\theta^*\)</span> that gets passed to
<span class="math">\(\mathcal{L}_{\text{dev}}\)</span>. And, <span class="math">\(\theta^*\)</span> is a function of <span class="math">\(\lambda\)</span>, denoted
<span class="math">\(\theta(\lambda)\)</span>. Furthermore, <span class="math">\(\textbf{argmin}\)</span> must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as</p>
<div class="math">$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$</div>
<p>
where <span class="math">\(\theta(\lambda)\)</span> is the solution to,
</p>
<div class="math">$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$</div>
<p>Now how does <span class="math">\(\theta\)</span> change as the result of an infinitesimal change to
<span class="math">\(\lambda\)</span>?</p>
<p>The constraint on the derivative implies a type of "equilibrium"---the inner
optimization process will continue to optimize regardless of how we change
<span class="math">\(\lambda\)</span>. Assuming we don't change <span class="math">\(\lambda\)</span> too much, then the inner
optimization shouldn't change <span class="math">\(\theta\)</span> too much and it will change in a
predictable way.</p>
<p>To do this, we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are related through a
function <span class="math">\(g\)</span> as follows,</p>
<div class="math">$$g(x,y) = 0.$$</div>
<p>Assuming <span class="math">\(g\)</span> is a smooth function in <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, we can perturb either
argument, say <span class="math">\(x\)</span> by a small amount <span class="math">\(\Delta_x\)</span> and <span class="math">\(y\)</span> by <span class="math">\(\Delta_y\)</span>. Because
system preserves the constraint, i.e.,</p>
<div class="math">$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$</div>
<p>We can solve for the change of <span class="math">\(x\)</span> as a result of an infinitesimal change in
<span class="math">\(y\)</span>. We take the first-order expansion,</p>
<div class="math">$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$</div>
<p>Since <span class="math">\(g(x,y)\)</span> is already zero,</p>
<div class="math">$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$</div>
<p>Next, we solve for <span class="math">\(\frac{\Delta_x}{\Delta_y}\)</span>.</p>
<div class="math">$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$</div>
<div class="math">$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$</div>
<p>Back to the original problem: Now we can use the implicit function theorem to
estimate how <span class="math">\(\theta\)</span> varies in <span class="math">\(\lambda\)</span> by plugging in <span class="math">\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)</span>, <span class="math">\(x \mapsto \theta\)</span>
and <span class="math">\(y \mapsto \lambda\)</span>:</p>
<div class="math">$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$</div>
<p>This tells us how <span class="math">\(\theta\)</span> changes with respect to an infinitesimal change to
<span class="math">\(\lambda\)</span>. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt <span class="math">\(\lambda\)</span>,</p>
<div class="math">$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$</div>
<p>Since we don't like (explicit) matrix inverses, we compute <span class="math">\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)</span> as the solution to <span class="math">\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)</span>. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products---i.e., you never have to materialize the Hessian. (Apparently,
<a href="https://en.wikipedia.org/wiki/Matrix-free_methods">matrix-free linear algebra</a>
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
<a href="/blog/post/2014/02/10/gradient-vector-product/">earlier post</a>).</p>
<p>At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.</p>
<p>In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.</p>
<p><strong>Further reading</strong>:</p>
<ul>
<li>
<p><a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/">Truncated Bi-Level Optimization</a></p>
</li>
<li>
<p><a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf">Efficient multiple hyperparameter learning for log-linear models</a></p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1502.03492">Gradient-based Hyperparameter Optimization through Reversible Learning</a></p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Tim Vieira</span>
  </span>
<time datetime="2016-03-05T00:00:00-05:00" pubdate>Mar 05, 2016</time>  <span class="categories">
    <a class="category" href="../../../../../tag/math.html">math</a>
    <a class="category" href="../../../../../tag/calculus.html">calculus</a>
  </span>
</p><div class="sharing">
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="../../../../../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/" data-via="xtimv" data-counturl="../../../../../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/" >Tweet</a>
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="../../../../../post/2016/05/28/the-optimal-proposal-distribution-is-not-p/">The optimal proposal distribution is not p</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2016/05/27/dimensional-analysis-of-gradient-ascent/">Dimensional analysis of gradient ascent</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/">Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2016/01/17/multidimensional-array-index/">Multidimensional array index</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2015/07/29/gradient-of-a-product/">Gradient of a product</a>
      </li>
    </ul>
  </section>

<!--      
  <section>
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="../../../../../category/misc.html">misc</a></li>
    </ul>
  </section>
 
-->

  <section>
  <h1>Tags</h1>
    <a href="../../../../../tag/calculus.html">calculus</a>,    <a href="../../../../../tag/visualization.html">visualization</a>,    <a href="../../../../../tag/statistics.html">statistics</a>,    <a href="../../../../../tag/importance-sampling.html">importance-sampling</a>,    <a href="../../../../../tag/structured-prediction.html">structured-prediction</a>,    <a href="../../../../../tag/numerical.html">numerical</a>,    <a href="../../../../../tag/misc.html">misc</a>,    <a href="../../../../../tag/sampling.html">sampling</a>,    <a href="../../../../../tag/deep-learning.html">deep-learning</a>,    <a href="../../../../../tag/rant.html">rant</a>,    <a href="../../../../../tag/optimization.html">optimization</a>,    <a href="../../../../../tag/crf.html">crf</a>,    <a href="../../../../../tag/machine-learning.html">machine-learning</a>,    <a href="../../../../../tag/gumbel.html">Gumbel</a>,    <a href="../../../../../tag/randomized.html">randomized</a>,    <a href="../../../../../tag/math.html">math</a>  </section>



  <section>
    <h1>GitHub Repos</h1>
    <ul id="gh_repos">
      <li class="loading">Status updating...</li>
    </ul>
      <a href="https://github.com/timvieira">@timvieira</a> on GitHub
    <script type="text/javascript">
      $.domReady(function(){
          if (!window.jXHR){
              var jxhr = document.createElement('script');
              jxhr.type = 'text/javascript';
              jxhr.src = '../../../../../theme/js/jXHR.js';
              var s = document.getElementsByTagName('script')[0];
              s.parentNode.insertBefore(jxhr, s);
          }

          github.showRepos({
              user: 'timvieira',
              count: 4,
              skip_forks: true,
              target: '#gh_repos'
          });
      });
    </script>
    <script src="../../../../../theme/js/github.js" type="text/javascript"> </script>
  </section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      var widgetId = '551816176788328448',
          domId = 'tweets',
          count = 3,
          hyperlinkUrlsPlus = true,
          showUserPhoto = false,
          timeOfTweet = true,
          dateFormat = 'default',
          showRetweets = false,
          customOutputFn = null,
          showReplyRetweetButtons = false;

      twitterFetcher.fetch(
        widgetId,
        domId,
        count,
        hyperlinkUrlsPlus,
        showUserPhoto,
        timeOfTweet,
        dateFormat,
        showRetweets,
        customOutputFn,
        showReplyRetweetButtons
      );
    });
  </script>
  <script src="/theme/js/twitter.js" type="text/javascript"> </script>
    <a href="http://twitter.com/xtimv" class="twitter-follow-button" data-show-count="true">Follow @xtimv</a>
</section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014-2016  - Tim Vieira -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
	<script type="text/javascript">
	  var disqus_shortname = 'graduatedescent';
          var disqus_identifier = '/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/';
          var disqus_url = '../../../../../post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/';
          var disqus_title = 'Gradient-based Hyperparameter Optimization and the Implicit Function Theorem';
	  (function() {
	    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	    dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
	    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	   })();
	</script>
  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>
</body>
</html>